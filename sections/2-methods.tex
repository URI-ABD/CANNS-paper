\section{Methods}
\label{sec:methods}

In this manuscript, we are primarily concerned with $k$-nearest neighbors in a finite-dimensional 
vector space. 

Given a dataset $\textbf{X} = \{x_1 \dots x_n\}$, we define a \emph{point} or \emph{datum} $x_i \in \textbf{X}$ as a singular observation (e.g., the genome of 
an organism, the vector representation of a single image, or any entity on which we can define a \emph{distance function}).

We define a \emph{distance function} $f : (\textbf{X}, \textbf{X}) \mapsto \mathbb{R}^+ \cup \{0\}$ as a function which, 
given two points $x, y \in \textbf{X}$, deterministically returns a non-negative real number. We require that the distance function 
be symmetric ($f(x, y) = f(y, x)$ for all $x, y \in \textbf{X}$) and that the distance between two points $x$ and $y$ be zero if and only if $x = y$. 
When, in addition to these constraints, the distance function obeys
the triangle inequality, it is a \emph{distance metric}, and in such cases we can guarantee that our search algorithms have perfect recall. 
Choice of distance function varies by type of data. For example, with electromagnetic spectra, we use both 
Euclidean (L2) and Cosine distances. With biological or string data, Hamming and Levenshtein distances are more appropriate.


Some of our algorithms for $\rho$- and $k$-nearest neighbors search rely on the \emph{local fractal dimension} at some point in the dataset, 
which we define as: 
\begin{equation} \frac{\text{log}(\frac{|B_X(q, r_1)|}{|B_X(q, r_2)|})}{\text{log}(\frac{r_1}{r_2}) } \label{1} \end{equation}
where $B_X(q, r)$ is the set of points contained in a ball of radius $r$ 
centered at a point $q$ in the dataset $\textbf{X}$; in this example, we compute fractal dimension for some radius $r_1$ and a smaller radius $r_2 = \frac{r_1}{2}$.
We stress that this concept differs from the \emph{embedding dimension} of a dataset. To illustrate the difference,
consider SDSS's APOGEE dataset, wherein each datum is a nonnegative real-valued vector of length 8,575. Hence, the \emph{embedding dimension} of this dataset is 8,575. 
However, due to physical constraints (namely, the laws that govern stellar fusion and spectral emission lines), the data are constrained to a lower-dimensional 
manifold within the 8,575-dimensional embedding space. The \emph{local fractal dimension} is an approximation of the dimensionality of that lower-dimensional manifold at a given point, for some length scale.
The notion that high-dimensional data collected from constrained generating phenomena typically only occupy a low-dimensional manifold is known as the \emph{manifold hypothesis}.
[TODO: add example for GreenGenes and add lfd plots from CHESS]

We define a \emph{cluster} as a set of points with a \emph{center}, a \emph{radius}, and an approximation of the \emph{local fractal dimension}.
The \emph{center} is the geometric median of a sample of points in the \emph{cluster}, and so it is a real data point. The \emph{radius} is the
maximum distance from a point in the cluster to its \emph{center}. We estimate \emph{local fractal dimension} at the cluster radius and half
the cluster radius. Each cluster (unless it is a leaf cluster) has two child clusters in much the same way that a node in
a binary tree has two child nodes. We define the \emph{metric entropy} of a data set under a hierarchical clustering scheme as a refinement of [7], where
metric entropy for a given cluster radius $r_c$ was the number of clusters of radius $r_c$ needed to cover all data. Here, we use a hierarchical, divisive clustering 
approach, but with early stopping criteria; clusters which satisfy a user-specified stopping criterion (e.g. a specified cardinality or radius) are not further split. 
Since we frame the asymptotic complexity of $\rho$-NN search in terms of the number of leaf clusters, the \emph{metric entropy} is best thought of in terms of the number of
leaf clusters. 


With these concepts defined, we can now pose the $k$- and $\rho$ nearest neighbors search problems.
Given a query $q$, along with a distance function $f$ defined on a dataset $\textbf{X}$, $k$-nearest neighbors search aims to find 
the set $\{x \in \textbf{X}: f(q, x) \leq f(q, x_k) \land x_k \text{ is the $k$th nearest neighbor of $q$ in $\textbf{X}$}\}$; that is,
for a given $k$, find the $k$ closest points to $q$ in $ \textbf{X}$.
We also have the $\rho$-nearest neighbors search problem, which aims to find the set $\{x \in \textbf{X}: f(q, x) \leq \rho \}$; that is, 
find all points in $\textbf{X}$ that at most a distance $\rho$ from $q$.

Given a Cluster $C$, let $c$ be its center and $r$ be its radius. Our $\rho$- and $k$-NN algorithms make use of the following cluster 
properties:
\begin{itemize}
    \item $\delta = f(q, c)$ is the distance from the query to the cluster center $c$.
    \item $\delta_{max} = \delta + r$ is the distance from the query to the theoretically farthest possible instance in $C$.
    \item $\delta_{min} = \text{max}(0, \delta - r)$ is the distance from the query to the theoretically closest possible instance in $C$.
\end{itemize}


We define \emph{singletons} as clusters which either contain a single point (i.e., have cardinality 1) or which contain 
many instances of the same point (i.e., have cardinality greater than 1 but contain only one unique point). Singletons clearly 
have zero radius, and so $\delta_{max} = \delta_{min} = \delta$ for these clusters. Hence, we sometimes overload the above 
notation to refer to distance from a query to a point. Similarly, we have that $\delta_{max} = \delta = \delta_{min}$ for 
individual points.


\subsection{Clustering}
\label{subsec:methods:clustering}

We start by building a divisive hierarchical clustering of the dataset with CLAM, using a 
similar recursive procedure as outlined in CHESS~\cite{ishaq2019clustered}, but with the following 
improvements: better selection of poles for partitioning and depth-first dataset reordering 
(see Section ADD SECTION: Dataset Reordering). 


CLAM assumes the manifold hypothesis. 
In other words, we assume that the dataset is embedded in a $D$-dimensional space, but that the data only occupy 
a $d$-dimensional manifold, where $d \ll D$. 
While we sometimes use Euclidean notions, such as voids and volumes, to talk about geometric and topological 
properties of clusters and of the manifold, CLAM does not rely on such notions; 
they serve merely as a convenient, more intuitive way to talk about the underlying mathematics.


For a cluster $C$ with $|C|$ points, we begin by computing a 
random sample of $\sqrt{|C|}$ points, and computing pairwise distances 
between all points in the sample. Based on those pairwise distances, we compute the \emph{geometric median} of this sample; 
that is, the point which minimizes the sum of distances to all other points in the sample. This geometric median 
is $C$'s \emph{center}. The \emph{radius} of $C$ is the maximum distance from any point in $C$ to its center.
The point $l$ which is furthest from the center is designated the \emph{left pole}, and the point $r$ which is furthest
from $l$ is designated the \emph{right pole}. We then partition the cluster into a \emph{left child} and \emph{right child}, where the 
left child contains all points in the cluster which are closer to $l$ than to $r$, and the right child contains all 
points in the cluster which are closer to $r$ than to $l$. Without loss of generality, we assign to the left child 
those points which are equidistant from $l$ and $r$. Starting from a root-cluster containing the entire dataset, we 
repeat this procedure until each leaf contains only one datum or until some other user-specified stopping criterion 
is met.


\begin{algorithm} % enter the algorithm environment
\caption{Partition} % give the algorithm a caption
\label{alg:partition} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $cluster$
    \STATE $m \leftarrow \lfloor \sqrt{|cluster.points|} \rfloor$
    \STATE $seeds \leftarrow m$ random points from $cluster.points$
    \STATE $c \leftarrow$ geometric median of $seeds$
    \STATE $l \leftarrow \argmax d(c,x) \ \forall \ x \in cluster.points$
    \STATE $r \leftarrow \argmax d(l,x) \ \forall \ x \in cluster.points$
    \STATE $left \leftarrow \{x | x \in cluster.points \land d(l,x) \le d(r,x)\}$
    \STATE $right \leftarrow \{x | x \in cluster.points \land d(r,x) < d(l,x)\}$
    \IF{$|left| > 1$}
        \STATE Partition($left$)
    \ENDIF
    \IF{$|right| > 1$}
        \STATE Partition($right$)
    \ENDIF
\end{algorithmic}
\end{algorithm}

\subsubsection {Dataset Depth-First Reordering}

CLAM-CAKES also reorders the dataset to improve the tree-building process 
used in CHESS. In CHESS, each cluster stored a vector of indices of its points. 
We use these indices during search to retrieve a cluster's points from the dataset. 
Though this approach allowed us to retrieve the points in constant 
time, its memory cost was prohibitively high; since each cluster stores indices of all of its
points, for a dataset with cardinality $n$, we store a total of $n$ indices at each depth in the tree.
With $\log{}n$ levels in the tree, this approach had $\mathcal{O}(n\log{}n)$ memory cost. 


A later improvement reduced the memory cost to $\mathcal{O}(n)$ by storing only the indices of the points in the leaves.
This approach, however, introduced an $\mathcal{O}(n)$ time cost every time we needed to find the indices for 
a non-leaf cluster, since this required a traversal of the subtree rooted at that cluster.


In this work, we introduce a new approach, wherein after building the cluster tree, we reorder the dataset 
so that points are stored in a depth-first order based on the tree. Then, within each cluster, we need only 
store the cluster's cardinality, and an \emph{offset} to access the cluster's points from the dataset.
For left leaf clusters, the offset is the index of the first point in the cluster. Right leaves have an offset 
equal to their sibling's offset plus the their sibling's cardinality. Parents have the same offset as their left child.
With $\mathcal{O}(n)$ memory cost and $\mathcal{O}(1)$ time cost for retrieving a points during search, 
dataset depth-first reordering offers significant improvements over the previous approaches.


\subsubsection {Guaranteed Decrease in Cluster Radii}
\label{subsubsec:methods:guaranteed-decrease-in-cluster-radii}

TODO: This needs some doctoring. 

We show that cluster radius is guaranteed to decrease after at most $d$ partitions.

Assume that we have a dataset which follows a $d$-dimensional distribution embedded in $D$-dimensional space.
We can describe this distribution choosing some set of $d$ mutually orthogonal axes.
Let $m$ denote the maximum distance between any pair of points along any axis from any choice of mutually orthogonal axes for our $d$-dimensional distribution.
In the worst case scenario, the distribution of our data is a $d$-sphere and $m$ is the maximum distance between two points along every axis for every choice of axes.
In this scenario, the cluster radius is $r = \frac{m}{2}$.

Given a cluster $C$, we choose the left and right poles, 
and the axis defined by those poles will be the axis for partitioning  $C$'s points into two child clusters.
For points in each child cluster, the maximum distance between any pair of points along the \emph{axis of partitioning} will have been reduced to at most $\frac{m}{2}$.
Though this bound holds for points on the axis of partitioning, it does not serve as a bound for the maximum distance between \emph{any} pair of points in  $C$.
To see this, consider the two dimensional scenario where the points in  $C$ are distributed in a circle of radius $r$. Clearly, the maximum possible distance between 
any two points in  $C$ is $2r$, and occurs when the points fall on opposite ends of one of its diameters. Now consider a horizontal diameter of the circle, 
and let $l$ denote the leftmost point on this diameter. Next, consider the orthogonal vertical diameter, and let $x$ denote the uppermost point on this diameter. When we 
partition along the horizontal diameter, $l$ and $x$ will be assigned to the left child (as mentioned in Section 2, we assign equidistant points to the left child). 
This reduces the maximum possible distance between any two points on the axis of partition --the horizontal diameter-- to $r$. However, by considering the 
right triangle formed by $l$, $x$, and the center of the circle, we can see that the distance between 
$l$ and $x$ is $\sqrt{2}{r}$.


If $d > 1$ then the child clusters will have a maximal chord $m_{child}$ such that $m_{child} \leq m$.
By partitioning the cluster in this way, we have taken one axis and reduced the maximum distance along that axis from $m$ to $\frac{m}{2}$.
However, as per the distribution of data as discussed earlier, there may be other axes along which the maximum pairwise distance is still $m$.
We, thus, recursively partition each child cluster.

With each recursive application of Partition, we consume one axis along which the maximum distance was $m$ and reduce the maximum distance along that axis to $\frac{m}{2}$ in the child clusters.
After $d$ recursive applications of partition, we will have exhausted the $d$ axes with the maximum distance of $m$.
After these $d$ partitions, the points in each child cluster will have a maximum pairwise distance of $\frac{m}{2}$ along any axis.
Thus the radii of those child clusters will be at most $\frac{m}{4}$.

\subsubsection {Complexity}
\label{subsubsec:methods:clustering:complexity}

Clustering: Exact partition with $\mathcal{O}(n^2)$ cost vs approximate partition using $\sqrt{n}$ seeds to achieve $\mathcal{O}(n)$ cost.
Building the exact tree costs $\mathcal{O}(n^2 \log n)$ vs approximate tree for $\mathcal{O}(n \log n)$.


\subsection {Index-Building}
\begin{itemize}
    \item sharded search
    \item index building 
    \item optimization of number of shards 
\end{itemize}

\subsection{\texorpdfstring{$\rho$}{p}-Nearest Neighbors Search}
\label{subsec:methods:rnn-search}


[TODO: delete this and talk about overlapping children improvement instead]
We conduct $\rho$-nearest neighbors search as described in CHESS~\cite{ishaq2019clustered}, but 
with some implementation improvements including identification of candidate clusters which are leaves and 
conducting exhaustive search over those leaves.

\begin{algorithm} 
    \caption{$\rho$-NN(\emph{clusters, query, r})} 
    \label{alg:rnn} 
    \begin{algorithmic}[2]
        \REQUIRE $r \geq 0$
        \REQUIRE $clusters \neq \emptyset$
        \STATE $results \leftarrow \emptyset$
        \IF{$clusters.left$}
            \IF{$clusters.left.\delta$ $\leq$ $r$ + $clusters.left.radius$}
                \STATE $\rho$-NN($clusters.left, query, r$)
            \ENDIF
        \ENDIF
        \IF{$clusters.right$}
            \IF{$clusters.right.\delta$ $\leq$ $r$ + $clusters.right.radius$} 
                \STATE $\rho$-NN($clusters.right, query, r$)
            \ENDIF
        \ENDIF
        \IF{$\neg$$clusters.left$ $\land \neg$$clusters.right$}
            \FOR{$p \in clusters.points$}
                \IF{$p.\delta \leq r$}
                    \STATE $results \leftarrow r$
                \ENDIF
            \ENDFOR
        \ENDIF
        \STATE Return $results$
    \end{algorithmic}
    \end{algorithm}

The asymptotic complexity of $\rho$-nearest neighbors is the same as in CHESS~\cite{ishaq2019clustered}.

\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{subsec:methods:knn-search}

[TODO: reorder so rho nn is first, then sieves, then Greedy Sieve]
In this section, we present four novel algorithms for $k$-nearest neighbors search: Greedy Search, $k$-NN by Repeated $\rho$-NN, Sieve Search I, and Sieve Search II. 
We also have an implementation of linear search. We use a preliminary auto-tuning function to predict the variant which will perform 
best for a given query, dataset, and value of $k$. We then proceed with search using that variant.  


[TODO: tighten up or omit those opening paragraphs which roughly explain the algorithm]


[TODO: call queues of points H]
\subsubsection{Greedy Search}
\label{subsubsec:methods:knn-search:greedy-search}
Greedy Search repeatedly partitions the cluster with the smallest $\delta_{min}$ into its children 
until the cluster with the smallest $\delta_{min}$ is a leaf. We then remove it and add its points to a fixed size priority queue of points. 
Search terminates when the furthest hit in the queue of points has $\delta$ greater than the $\delta_{min}$ of the closest remaining cluster.

Formally, let $Q$ be a priority queue of clusters sorted by non-increasing $\delta_{min}$. Initialize $Q$ with the root cluster.
Let $P$ be a second priority queue of points sorted by non-decreasing distance from the query. This queue starts empty.

[TODO: replace with pop and push language and start with initial pop]
While $P$ has fewer than $k$ clusters or $\delta$ of the top priority element in $P$ is greater 
than $\delta_{min}$ of the top priority element in $Q$, do the following:
\begin{enumerate}
\item While the top priority element $q$ in $Q$ is not a leaf, replace $q$ with its children.
\item Remove $q$ from $Q$ and add all of its points to $P$. 
\item Remove points from $P$ until $P$ has $k$ points. 
\end{enumerate}
After this process, the points left in $P$ are the $k$ nearest neighbors of $q$.

[TODO: fix to use priority queue language]

\begin{algorithm} 
\caption{GreedySearch(\emph{tree, query, k})} 
\label{alg:greedy_search} 
\begin{algorithmic}[3]
    \STATE $Q \leftarrow$ priority queue
    \STATE $Q.push(tree.root)$
    \STATE $P \leftarrow$ priority queue
    \WHILE{$|P| < k \lor P.max.\delta > Q.min.\delta_{min}$}
        \WHILE{$\neg Q.min.isLeaf$}
            \STATE $[l, r] \leftarrow Q.extractMin().children$
            \STATE $Q.push([l, r])$
        \ENDWHILE
        \STATE $leaf \leftarrow Q.extractMin()$
        \STATE $P.push(leaf)$
        \WHILE{$|P| > k$}
            \STATE $P.extractMax()$
        \ENDWHILE
    \ENDWHILE
    \STATE Return $P$
\end{algorithmic}
\end{algorithm}


\subsubsection{$k$-NN by Repeated $\rho$-NN}
\label{subsubsec:methods:knn-search:repeated-rnn}
In this algorithm, we perform
$\rho$-nearest neighbors search at [TODO: add radius], repeatedly increasing the search radius until $k$ neighbors
have been found.

[TODO: change P to H]
Let $P$ be the set of nearest neighbors found thus far.
Search starts with a radius $m$ equal to the radius of the cluster tree divided by
the cardinality of the dataset. We perform $\rho$-NN search with radius $m$. 
If no points are within a distance $m$ of the query, we increase the radius by a factor of 
2 and perform $\rho$-NN search again, repeating until at least one point is found, i.e., 
until $|P| > 0$.


[TODO: Add number next to all equations and fix factor so that it is correct (k should be raised too)]
Once $|P| > 0$, we continue to perform $\rho$-NN search, but instead of 
increasing the radius by a factor of 2 on each iteration, we increase it by a factor determined 
by the local fractal dimension of the clusters containing the points found so far. In particular, 
we increase the radius by a factor of 
$$\text{min}\left(2, \frac{k}{|P|^{\frac{1}{\mu}}}\right)$$
where $\mu$ is the mean local fractal dimension (abbreviated lfd in algorithm below) of the clusters containing the points found so far.
Intuitively, the factor by which we increase the radius should be \emph{inversely} proportional to the number of points found so far. 
Additionally, when the local fractal dimension near the query and found neighbors is low, this suggests that the data 
are relatively concentrated in that region; thus, the factor of radius increase should be \emph{directly} proportional to the 
local fractal dimension. Since the local fractal dimension is memoized at clustering time, this poses no additional cost at search time.
Once $|P| >= k$, we sort the points in $P$ by $\delta$ and return the $k$ closest points.

\begin{algorithm} % enter the algorithm environment
    \caption{Repeated$\rho$-NN(\emph{tree, query, k})} % give the algorithm a caption
    \label{alg:knn-by-rnn} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[4] % enter the algorithmic environment
        \STATE $P \leftarrow$ $\emptyset$
        \STATE $m \leftarrow$ $\frac{tree.radius}{tree.cardinality}$
        \WHILE {$|P| = 0$}
            \STATE $P.push(\rho$-NN$(tree, query, m)$)
            \STATE $m \leftarrow 2m$
        \ENDWHILE
        \WHILE {$|P| < k$}
            \STATE $clusters \leftarrow \{ C: \exists p \in P \land p \in C \}$
            \STATE $\mu \leftarrow \frac{1}{|clusters|} \sum_{C \in clusters} C.lfd$
            \STATE $m \leftarrow \text{min}\left(2, \frac{k}{|P|^{\frac{1}{\mu}}}\right)$
            \STATE $P.push(\rho$-NN$(tree, query, m)$)
        \ENDWHILE
        \STATE $P.sort()$
        \STATE Return $P[0..k]$
    \end{algorithmic}
    \end{algorithm}


[TODO: change sieve II to sieev with separate centers]
\subsubsection{Sieve Search I}
\label{subsubsec:methods:knn-search:sieve}
[TODO: make writeup here tighter and more consistent with what's in the algorithm; talk 
about enum variants here]
With Sieve Search I, we repeatedly calculate a threshold distance $\tau$ such that no cluster with a $\delta_{min}$ greater than $\tau$ 
can contain one of the $k$ nearest neighbors. Search terminates when we have a threshold containing exactly $k$ points.

We start by letting $Q$ be a vector initialized with the root cluster. This vector will maintain the 
clusters and points which are still in contention for being one of the nearest neighbors.
While $Q$ contains at least one cluster (i.e., $Q$ is not a vector of only points), we repeat the process described in 
the following paragraphs. 

First, 
we aim to find the element $q_{\tau} \in Q$ with the smallest $\delta_{max}$ such that 
$q_{\tau}$ and all elements in $Q$ with smaller $\delta_{max}$ collectively contain at least $k$ points. 

To find this element, 
we use a process similar to the Partition algorithm used in Quicksort (CITE: original Quicksort paper and reference wherever it is 
in our paper), adjusted to account for the varying cardinalities of elements in $Q$. Our modified algorithm finds the smallest index $i$ such 
that all elements in $Q$ with $\delta_{max}$ closer to or equal to $Q[i]$'s $\delta_{max}$ collectively have cardinality greater
than or equal to $k$. In the process of finding this index, we alter the order of $Q$ so that all elements to left
of index $i$ have $\delta_{max}$ less than or equal to $Q[i]$'s $\delta_{max}$ and all elements to right of index $i$
have $\delta_{max}$ greater than $Q[i]$'s $\delta_{max}$. We consider $Q[i]$ to be our $q_{\tau}$, and our threshold 
$\tau$ to be $Q[i]$'s $\delta_{max}$.

We then remove from $Q$ any element whose $\delta_{min}$ is greater than $\tau$. Then, for each element left 
in $Q$, if it is a leaf or contains $k$ or fewer points, we remove it and all of its points to $Q$. Finally, we
replace all remaining non-leaf clusters in $Q$ with their children. 

We complete this process until $Q$ contains only points. At this point, we 
just use [INSERT ref to quicksort partition in paper] to find the $k$th 
nearest neighbor. Since this algorithm will also elements on the correct side of 
the index of the $k$th nearest neighbor, the first $k$ elements in $Q$ a are the $k$-nearest neighbors.


\begin{algorithm} % enter the algorithm environment
    \caption{QuicksortPartition(\emph{Q, k, l, r})} % give the algorithm a caption
    \label{alg:quicksort-partition} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[5] % enter the algorithmic environment
        \IF{$l \geq r$}
            \STATE Return $min(l, r)$
        \ELSE 
            \STATE $pivot = l + (r - l) / 2$
            \STATE $Q.swap(pivot, r)$
            \STATE $a \leftarrow l$
            \STATE $b \leftarrow l$
            \WHILE {$b < r$}
                \IF {$Q[b].\delta_{max} < Q[r].\delta_{max}$}
                    \STATE $Q.swap(a, b)$
                    \STATE $a \leftarrow a + 1$
                \ENDIF
                \STATE $b \leftarrow b + 1$
            \ENDWHILE
            \STATE $Q.swap(a, r)$
            \STATE $p \leftarrow r$
            \STATE $g \leftarrow \sum_{i=0}^{p} Q[i].cardinality$
            \IF {$g = k$}
                \STATE Return $p$
            \ELSIF {$g < k$}
                \STATE $QuicksortPartition(Q, k, p + 1, r)$
            \ELSE 
                \IF {$p > 0 \land g > k + Q[p - 1].cardinality$}
                    \STATE $QuicksortPartition(Q, k, l, p - 1)$
                \ELSIF {$p > 0 \land g = k + Q[p - 1].cardinality$}
                    \STATE Return $p - 1$
                \ELSE
                    \STATE Return $p$
                \ENDIF
            \ENDIF
        \ENDIF
    \end{algorithmic}
    \end{algorithm}

\begin{algorithm} % enter the algorithm environment
    \caption{Sieve(\emph{tree, query, k})} % give the algorithm a caption
    \label{alg:sieve} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[6] % enter the algorithmic environment
        \REQUIRE $tree$, $query$, $k$
        \STATE $Q \leftarrow$ [$tree.root$]
        \WHILE{$|Q| > k$}
            \STATE $i \leftarrow QuicksortPartition(Q, k, 0, |Q| - 1)$
            \STATE $\tau \leftarrow Q[i].\delta_{max}$
            \FOR {$q \in Q$}
                \IF {$q.\delta_{min} > \tau$}
                    \STATE $Q.remove(q)$
                \ENDIF
            \ENDFOR
            \FOR {$q \in Q$}
                \IF {$q.isLeaf \lor |q.cardinality| \leq k$}
                    \STATE $Q.push(q.points)$
                \ELSE
                    \STATE $[l, r] \leftarrow q.children$
                    \STATE $Q.push([l, r])$   
                \ENDIF
                \STATE $Q.remove(q)$
            \ENDFOR 
        \ENDWHILE
        \STATE Return $Q$
    \end{algorithmic}
    \end{algorithm}

\subsubsection{Sieve Search II}
\label{subsubsec:methods:knn-search:sieve2}
Sieve Search II is exactly the same as Sieve Search I, but with the following modification: clusters 
in $Q$ are represented twice-- once as their center and once as the rest of their points. 
Because we have that for any cluster $C$, $C.\delta \leq C.\delta_{max}$, representing the cluster 
center separately from the rest of the points causes the threshold $\tau$ to decrease more quickly, 
thus allowing us to eliminate some clusters from $Q$ earlier in the process.

\subsubsection{Complexity of $k$-NN Search}
\label{paragraph:methods:knn-complexity}
Add asymptotic complexity of various $k$-NN algorithms here.