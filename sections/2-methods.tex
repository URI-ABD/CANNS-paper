\section{Methods}
\label{sec:methods}

In this manuscript, we are primarily concerned with $k$-nearest neighbors in a finite-dimensional 
vector space. Given a dataset $\textbf{X} = \{x_1 \dots x_n\}$, we define a \emph{point} or \emph{datum} $x_i \in \textbf{X}$ as a singular observation (e.g., the genome of 
an organism, the vector representation of a single image, or any entity on which we can define a \emph{distance function}).

We define a \emph{distance function} $f : (\textbf{X}, \textbf{X}) \mapsto \mathbb{R}^+ \cup \{0\}$ as a function which, 
given two points $x, y \in \textbf{X}$, deterministically returns a non-negative real number. We require that the distance function 
be symmetric (i.e., $f(x, y) = f(y, x)$ for all $x, y \in \textbf{X}$) and that the distance between two points $x$ and $y$ be zero if and only if $x = y$. 
When, in addition to these constraints, the distance function obeys
the triangle inequality, it is a \emph{distance metric}, and in such cases we can guarantee that our search algorithms have perfect recall. 
Choice of distance function varies by type of data. For example, with electromagnetic spectra, we use both 
Euclidean (L2) and Cosine distances. With biological or string data, Hamming and Levenshtein distances are more appropriate.


Some of our algorithms for $\rho$- and $k$-nearest neighbors search rely on the \emph{local fractal dimension} at some point in the dataset, 
which we define as: 
\begin{equation} \frac{\text{log}(\frac{|B_X(q, r_1)|}{|B_X(q, r_2)|})}{\text{log}(\frac{r_1}{r_2}) } \label{1} \end{equation}
where $B_X(q, r)$ is the set of points contained in a ball of radius $r$ 
centered at a point $q$ in the dataset $\textbf{X}$; in this example, we compute fractal dimension for some radius $r_1$ and a smaller radius $r_2 = \frac{r_1}{2}$.
We stress that this concept differs from the \emph{embedding dimension} of a dataset. To illustrate the difference,
consider SDSS's APOGEE dataset, wherein each datum is a nonnegative real-valued vector of length 8,575. Hence, the \emph{embedding dimension} of this dataset is 8,575. 
However, due to physical constraints (namely, the laws that govern stellar fusion and spectral emission lines), the data are constrained to a lower-dimensional 
manifold within the 8,575-dimensional embedding space. The \emph{local fractal dimension} is an approximation of the dimensionality of that lower-dimensional manifold at a given point, for some length scale.
The notion that high-dimensional data collected from constrained generating phenomena typically only occupy a low-dimensional manifold is known as the \emph{manifold hypothesis}~\cite{fefferman2016testing}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=2.5in]{images/lfd_plots/lfd_plot_apogee.png}
    \caption{Mean fractal dimension of APOGEE clusters as a function of
    depth when clustered based on L2 norm. Each plot line represents a distinct
    decile of fractal dimension. Beyond a depth of 56, no clusters are further
    divided due to the minimum-cluster-cardinality stopping criteria.}
    \label{fig:methods:lfd_apogee}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=2.5in]{images/lfd_plots/lfd_plot_greengenes.png}
    \caption{Mean fractal dimension of GreenGenes clusters as a function of
    depth when clustered based on Hamming distance. Each plot line represents
    a distinct decile of fractal dimension.}
    \label{fig:methods:lfd_greengenes}
\end{figure}

Figure~\ref{fig:methods:lfd_apogee} shows the mean local fractal dimension of the APOGEE dataset at each level of hierarchical clustering, 
under the L2 (Euclidean) distance metric. Each plot line represents a decile of fractal dimension; note that, other than the most extreme 10\% of clusters,
virtually all clusters have a local fractal dimension of less than $2 \ll 8,575$. This suggests that APOGEE is a good candidate for use with CAKES. 


Figure~\ref{fig:methods:lfd_greengenes} shows the mean local fractal dimension of the GreenGenes dataset at each level of hierarchical clustering,
under the Hamming distance metric. Each plot line represents a decile of fractal dimension; once again, other than the most extreme 10\% of clusters,
virtually all clusters have a local fractal dimension of less than $2 \ll 2,250$. This suggests that GreenGenes is a good candidate for use with CAKES.



We define a \emph{cluster} as a set of points with a \emph{center}, a \emph{radius}, and an approximation of the \emph{local fractal dimension}.
The \emph{center} is the geometric median of a sample of points in the \emph{cluster}, and so it is a real data point. The \emph{radius} is the
maximum distance from a point in the cluster to its \emph{center}. We estimate \emph{local fractal dimension} at the cluster radius and half
the cluster radius. Each cluster (unless it is a leaf cluster) has two child clusters in much the same way that a node in
a binary tree has two child nodes. We define the \emph{metric entropy} of a data set under a hierarchical clustering scheme as a refinement of [7], where
metric entropy for a given cluster radius $r_c$ was the number of clusters of radius $r_c$ needed to cover all data. Here, we use a hierarchical, divisive clustering 
approach, but with early stopping criteria; clusters which satisfy a user-specified stopping criterion (e.g. a specified cardinality or radius) are not further split. 
Since we frame the asymptotic complexity of $\rho$-NN search in terms of the number of leaf clusters, the \emph{metric entropy} is best thought of in terms of the number of
leaf clusters. 


With these concepts defined, we can now pose the $k$- and $\rho$- nearest neighbors search problems.
Given a query $q$, along with a distance function $f$ defined on a dataset $\textbf{X}$, $k$-nearest neighbors search aims to find 
the set $S_q$ such that  $|S_q| = k$ and $\forall x \in X \setminus S_q$, $f(q, x) \leq \max\{f(y, q): y \in S_q \}$; that is,
for a given $k$, find the $k$ closest points to $q$ in $ \textbf{X}$.
We also have the $\rho$-nearest neighbors search problem, which aims to find the set $\{x \in \textbf{X}: f(q, x) \leq \rho \}$; that is, 
find all points in $\textbf{X}$ that are at most a distance $\rho$ from $q$.

Given a Cluster $C$, let $c$ be its center and $r$ be its radius. Our $\rho$- and $k$-NN algorithms make use of the following cluster 
properties:
\begin{itemize}
    \item $\delta = f(q, c)$ is the distance from the query to the cluster center $c$.
    \item $\delta_{max} = \delta + r$ is the distance from the query to the theoretically farthest possible instance in $C$.
    \item $\delta_{min} = \text{max}(0, \delta - r)$ is the distance from the query to the theoretically closest possible instance in $C$.
\end{itemize}


We define \emph{singletons} as clusters which either contain a single point (i.e., have cardinality 1) or which contain 
many instances of the same point (i.e., have cardinality greater than 1 but contain only one unique point). Singletons clearly 
have zero radius, and so $\delta_{max} = \delta_{min} = \delta$ for these clusters. Hence, we sometimes overload the above 
notation to refer to distance from a query to a point; in these cases, we also have that $\delta_{max} = \delta = \delta_{min}$.


\subsection{Clustering}
\label{subsec:methods:clustering}

We start by building a divisive hierarchical clustering of the dataset with CLAM, using a 
similar recursive procedure as outlined in CHESS, but with the following 
improvements: better selection of poles for partitioning and depth-first dataset reordering 
(see Section ~\ref{subsubsec:methods:dataset-depth-first-reordering}). 


CLAM assumes the manifold hypothesis. 
In other words, we assume that the dataset is embedded in a $D$-dimensional space, but that the data only occupy 
a $d$-dimensional manifold, where $d \ll D$. 
While we sometimes use Euclidean notions, such as voids and volumes, to talk about geometric and topological 
properties of clusters and of the manifold, CLAM does not rely on such notions; 
they serve merely as a convenient, more intuitive way to talk about the underlying mathematics.

\subsubsection {Cluster Partitioning}

For a cluster $C$ with $|C|$ points, we begin by computing a 
random sample of $\sqrt{|C|}$ of its points, and computing pairwise distances 
between all points in the sample. Based on those pairwise distances, we compute the \emph{geometric median} of this sample; 
that is, the point which minimizes the sum of distances to all other points in the sample. This geometric median 
is $C$'s \emph{center}. The \emph{radius} of $C$ is the maximum distance from any point in $C$ to its center.
The point $l$ which is responsible for that radius (i.e., the furthest point from the center) is designated the \emph{left pole}, and the point $r$ which is furthest
from $l$ is designated the \emph{right pole}. We then partition the cluster into a \emph{left child} and \emph{right child}, where the 
left child contains all points in the cluster which are closer to $l$ than to $r$, and the right child contains all 
points in the cluster which are closer to $r$ than to $l$. Without loss of generality, we assign to the left child 
those points which are equidistant from $l$ and $r$. Starting from a root-cluster containing the entire dataset, we 
repeat this procedure until each leaf contains only one datum or until some other user-specified stopping criterion 
is met.


\begin{algorithm} % enter the algorithm environment
\caption{Cluster Partition} % give the algorithm a caption
\label{alg:partition} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $cluster$
    \STATE $m \Leftarrow \lfloor \sqrt{|cluster.points|} \rfloor$
    \STATE $seeds \Leftarrow m$ random points from $cluster.points$
    \STATE $c \Leftarrow$ geometric median of $seeds$
    \STATE $l \Leftarrow \argmax d(c,x) \ \forall \ x \in cluster.points$
    \STATE $r \Leftarrow \argmax d(l,x) \ \forall \ x \in cluster.points$
    \STATE $left \Leftarrow \{x | x \in cluster.points \land d(l,x) \le d(r,x)\}$
    \STATE $right \Leftarrow \{x | x \in cluster.points \land d(r,x) < d(l,x)\}$
    \IF{$|left| > 1$}
        \STATE Partition($left$)
    \ENDIF
    \IF{$|right| > 1$}
        \STATE Partition($right$)
    \ENDIF
\end{algorithmic}
\end{algorithm}

\subsubsection {Dataset Depth-First Reordering}
\label{subsubsec:methods:dataset-depth-first-reordering}

CAKES also improves upon CHESS by reordering the dataset. In CHESS, each cluster stored a list of indices into the dataset, 
which we used during search to retrieve a cluster's points from the dataset. 
Though this approach allowed us to retrieve the points in constant 
time, its memory cost was prohibitively high; since each cluster stores indices for each of its
points, for a dataset with cardinality $n$, we store a total of $n$ indices at each depth in the tree.
With $\log{}n$ levels in the tree, this approach had $\mathcal{O}(n\log{}n)$ memory cost. 


One may think to improve this memory cost to $\mathcal{O}(n)$ by storing only the indices of points at 
the leaf cluster level.
This approach, however, introduces an $\mathcal{O}(n)$ time cost whenever we need to find the indices for 
a non-leaf cluster, since it a traversal of the subtree rooted at that cluster.


In this work, we introduce a new approach, wherein after building the cluster tree, we reorder the dataset 
so that points are stored in a depth-first order. Then, within each cluster, we need only 
store the cluster's cardinality and an \emph{offset} to access the cluster's points from the dataset. Parent clusters 
have the same offset as their left child, and right child clusters have an offset equal to their sibling's offset
plus their siblings's cardinality. With no additional memory cost nor time cost for retrieving a points during search, 
dataset depth-first reordering offers significant improvements over the approach used in CHESS.


\subsubsection {Scaling Behavior of Cluster Radii}
\label{subsubsec:methods:guaranteed-decrease-in-cluster-radii}
While it may be tempting to assume that cluster radii decrease with each application of Cluster Partition (refer to Algorithm \ref{alg:partition}), unfortunately, this assumption is incorrect. 
Fortunately, we \emph{can} make some guarantees about the scaling behavior of cluster radii; in particular, we can guarantee that cluster radii will stop increasing after at most 
$d$ partitions, where $d$ is the dimensionality of the dataset.  In the remainder of this subsection, we prove this guarantee. 


We can describe a $d$-dimensional distribution by choosing some set of $d$ mutually orthogonal axes.
Let $2R$ be the maximum pairwise distance among the instances in the dataset. 
We choose the axes such that the two points that are $2R$ apart lie along one of the axes. 
Thus, a $d$-dimensional hyper-sphere of radius $R$ would bound the dataset. 
In the worst case, (i.e., with a uniform-density distribution that fills the $d$-sphere), our axes will be such that $2R$ is the maximum pairwise distance along every axis. 
Such a distribution would also produce a balanced clustering.


Partition will select a maximally distant pair of points to use as poles, i.e., it will choose one of the $d$ axes along
which to split the cluster into two children. 
After one application of Algorithm \ref{alg:partition}, the maximum pairwise distance along that axis will be
bounded above by $R$. 
The next recursive Partition will select another of the $d$ axes. 
Thus, after at most $d$ applications of Algorithm \ref{alg:partition}, the
maximum pairwise distance along each axis will be bounded above by $R$. 
The overall (i.e., not restricted to be along one axis) maximum pairwise distance 
will be bounded above by $R\sqrt{2}$ by, for example, two instances that lie at the extrema of different axes. 

Thus, starting with a cluster $C$ of radius $R$, after at most $d$ Partitions, the descendants of $C$ will each have radius
bounded above by $\frac{R}{\sqrt{2}}$. In other words, cluster radii are guaranteed to decrease by a multiplicative factor of $\frac{\sqrt{2}}{2}$ after at 
most $d$ applications of Algorithm \ref{alg:partition}. 


Note that, in practice, we never see a balanced clustering. Cluster Partition  produces unbalanced trees due to the varying density of the sampling 
in different regions of the manifold and the low dimensional "shape" of the manifold. Further, the cluster radii decrease by a factor much larger than 
$\frac{\sqrt{2}}{2}$ in practice, and the upper bound of $d$ applications of Algorithm \ref{alg:partition} is seldom realized. 


\begin{figure}[ht!]
    \centering
    \includegraphics[width=2.5in]{images/geometry/geometry.pdf}
    \caption{Scaling Behavior of Radii in a Two-Dimensional Uniform-Density Disk}
    \label{fig:methods:scaling_behavior}
\end{figure}

\subsubsection {Complexity}
\label{subsubsec:methods:clustering:complexity}
%A lot of this is lifted from the CHESS paper and could probably get cut out. % 

To analyze the computational complexity of hierarchical
clustering, we start by considering the top level of the hierarchy. 
For a data set with cardinality $n$, we have $n$ data points at this 
level, from which we randomly choose a sample of $\sqrt{n}$ points. 
We compute distances between every pair of seeds in this sample, 
thus making $n$ distance comparisons. Based on these distances 
we compute the center and left and right poles. Once the poles 
have been chosen, every data point except for the two poles is 
compared to both poles, for an additional 2$n$ comparisons. 


In general, at depth $d$ of
the hierarchical tree, we have $\frac{n}{2^d}$ points per cluster (assuming a
perfectly balanced clustering), $\sqrt{\frac{n}{2^d}}$ seeds, $\frac{n}{2^d}$
pairwise seed comparisons, and $\frac{2n}{2^{d}}$ additional comparisons when 
each other data point is compared to the two poles, for a total of $\frac{3n}{2^d}$
distance comparisons. At each level, there are $2^d$ clusters,
so the total number of distance comparisons per level is
3$n$. Thus, the asymptotic complexity in terms of distance
comparisons for a cluster tree of depth $d$ is $\mathcal{O}(dn)$

By using an approximate partitioning with a $\sqrt{n}$ sample, we achieve $\mathcal{O}(n)$ cost of 
partitioning and $\mathcal{O}(n \log n)$ cost of building the tree. This is a significant improvement over 
exact partitioning and tree-building, which have $\mathcal{O}(n^2)$ and $\mathcal{O}(n^2 \log n)$ costs respectively.


\subsection {Index-Building and Sharded Search}
In this work, we also introduce the method of \emph{sharding} for $k$-nearest neighbors search.
With this approach, we aim to leverage the fact that $\rho$-nearest neighbors search is typically 
much faster than $k$-nearest neighbors search. 


We begin by randomly partitioning the dataset into $s$ shards, where $s$ is determined by [TODO: how did 
we decide we're going to select $s$ for this version of the paper?]. On the first shard, we perform 
$k$-nearest neighbors search using one of the four algorithms described in Section ~\ref{subsec:methods:knn-search}.
The result of this search will give us the $k$ closest neighbors to the query in the first shard. We then take the furthest of 
those $k$ neighbors and use its distance from the query as the radius for $\rho$-nearest neighbors search (see Section ~\ref{subsec:methods:rnn-search}) 
on each remaining shard. We then take the union of the results of these $\rho$-nearest neighbors searches, along with the $k$ neighbors
from the first shard, and then linear search over those points to find the actual $k$ nearest neighbors.


Note that the size of the shards is not necessarily consistent; it is often optimal to have the first shard be smaller than the others,
since the first shard is the only one for which we perform $k$-nearest neighbors search. However, making this first shard too small
creates the risk of significantly overestimating the radius, resulting in much slower $\rho$-nearest neighbors search on the remaining shards.
Finding the precise optimal ratio between the size of the first shard and the size of the remaining shards, as well 
as computing the expected level of error between the distance from the $k$-th closest neighbor in the first shard and the 
actual $k$-th closest neighbor, are topics for future work.

\subsection{\texorpdfstring{$\rho$}{p}-Nearest Neighbors Search}
\label{subsec:methods:rnn-search}

We conduct $\rho$-nearest neighbors search as described in \cite{ishaq2019clustered}, but 
with the following improvement: when a cluster overlaps with the query ball, instead of  
always proceeding with both of its children, we only proceed with those children which 
also overlap with the query ball. 

\begin{algorithm} 
    \caption{$\rho$-NN(\emph{clusters, query, r})} 
    \label{alg:rnn} 
    \begin{algorithmic}[2]
        \REQUIRE $r \geq 0$
        \REQUIRE $clusters \neq \emptyset$
        \STATE $results \Leftarrow \emptyset$
        \IF{$clusters.left \land clusters.left \cap B_X(query, r) \neq \emptyset$}
            \IF{$clusters.left.\delta$ $\leq$ $r$ + $clusters.left.radius$}
                \STATE $\rho$-NN($clusters.left, query, r$)
            \ENDIF
        \ENDIF
        \IF{$clusters.right \land clusters.right \cap B_X(query, r) \neq \emptyset$}
            \IF{$clusters.right.\delta$ $\leq$ $r$ + $clusters.right.radius$} 
                \STATE $\rho$-NN($clusters.right, query, r$)
            \ENDIF
        \ENDIF
        \IF{$\neg$$clusters.left$ $\land \neg$$clusters.right$}
            \FOR{$p \in clusters.points$}
                \IF{$p.\delta \leq r$}
                    \STATE $results.push((p, r))$
                \ENDIF
            \ENDFOR
        \ENDIF
        \STATE Return $results$
    \end{algorithmic}
    \end{algorithm}


The asymptotic complexity of $\rho$-nearest neighbors is the same as in ~\cite{ishaq2019clustered}.

\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{subsec:methods:knn-search}

In this section, we present four novel algorithms for $k$-nearest neighbors search: $k$-NN by Repeated $\rho$-NN, Sieve Search, and Sieve Search with Separate Centers, 
and  Greedy Sieve. 
We also have an implementation of linear search. We use a preliminary auto-tuning function to predict the variant which will perform 
best for a given query, dataset, and value of $k$. We then proceed with search using that variant.  

In these algorithms, we typically use $H$, for \emph{hits}, to refer to the data structure which stores the closest points to the query found so far and
$Q$ to refer to the data structure which stores the clusters and points which are still in contention for being one of the nearest neighbors.


\subsubsection{$k$-NN by Repeated $\rho$-NN}
\label{subsubsec:methods:knn-search:repeated-rnn}


In this algorithm, we perform $\rho$-nearest neighbors search at a radius equal to the radius of the root cluster divided by
the cardinality of the dataset, repeatedly increasing the search radius until $k$ neighbors
have been found.

Let $H$ be the set of nearest neighbors found thus far.
Search starts with a radius $m$ equal to the radius of the root cluster divided by
the cardinality of the dataset. We perform $\rho$-NN search with radius $m$. 
If no points are within a distance $m$ of the query, we increase the radius by a factor of 
2 and perform $\rho$-NN search again, repeating until at least one point is found, i.e., 
until $|H| > 0$.


Once $|H| > 0$, we continue to perform $\rho$-NN search, but instead of 
increasing the radius by a factor of 2 on each iteration, we increase it by a factor determined 
by the local fractal dimension of the clusters containing the points found so far. In particular, 
we increase the radius by a factor of 
\begin{equation} \text{arg min}\left(2, \left({\frac{k}{|H|}}\right)^{\frac{1}{\mu}}\right) \label{2} \end{equation}
where $\mu$ is the local fractal dimension (abbreviated lfd in algorithm below) in the region around the query point.
Intuitively, the factor by which we increase the radius should be \emph{inversely} related to the number of points found so far. 
Additionally, when the local fractal dimension at the radius scale from the previous iteration is low, this suggests that the data 
are relatively concentrated in that region; thus, the factor of radius increase should be \emph{directly} related to the 
local fractal dimension. Once $|H| >= k$, we return the $k$ closest points.

\begin{algorithm} % enter the algorithm environment
    \caption{Repeated$\rho$-NN(\emph{root, query, k})} % give the algorithm a caption
    \label{alg:knn-by-rnn} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[4] % enter the algorithmic environment
        \STATE $H \Leftarrow$ $\emptyset$
        \STATE $m \Leftarrow$ $\frac{root.radius}{|root|}$
        \WHILE {$|H| = 0$}
            \STATE $H.push(\rho$-NN$(root, query, m)$)
            \STATE $m \Leftarrow 2m$
        \ENDWHILE
        \WHILE {$|H| < k$}
            \STATE $Q \Leftarrow \{ C: \exists p \in H \land p \in C \}$
            \STATE $\mu \Leftarrow \frac{1}{|Q|} \sum_{C \in Q} C.lfd$
            \STATE $m \Leftarrow \text{arg min}\left(2, \left({\frac{k}{|H|}}\right)^{\frac{1}{\mu}}\right)$
            \STATE $H.push(\rho$-NN$(root, query, m)$)
        \ENDWHILE
        \STATE $H.sort()$
        \STATE Return $H[0], H[1], \cdots H[k-1]$
    \end{algorithmic}
    \end{algorithm}


\subsubsection{Sieve Search}
\label{subsubsec:methods:knn-search:sieve}
With Sieve Search, we begin by letting $Q$ be a list initialized with the root cluster. This list will maintain the 
clusters and points which are still in contention for being one of the nearest neighbors.
While $Q$ contains at least one cluster (i.e., $Q$ is not a list of only points), we repeat the process described in 
the following paragraphs. 

First, 
we aim to find the element $q_{\tau} \in Q$ with the smallest $\delta_{max}$ such that 
$q_{\tau}$ and all elements in $Q$ with smaller $\delta_{max}$ collectively contain at least $k$ points. 

To find this element, 
we use a process similar to the Partition algorithm used in Quicksort~\cite{10.1093/comjnl/5.1.10}, adjusted to account for the varying cardinalities of elements in $Q$. Our modified algorithm finds the smallest index $i$ such 
that all elements in $Q$ with $\delta_{max}$ closer to or equal to $Q[i]$'s $\delta_{max}$ collectively have cardinality greater
than or equal to $k$. In the process of finding this index, we alter the order of $Q$ so that all elements to left
of index $i$ have $\delta_{max}$ less than or equal to $Q[i]$'s $\delta_{max}$ and all elements to right of index $i$
have $\delta_{max}$ greater than $Q[i]$'s $\delta_{max}$. We consider $Q[i]$ to be our $q_{\tau}$, and our threshold 
$\tau$ to be $Q[i]$'s $\delta_{max}$.

We then remove from $Q$ any element whose $\delta_{min}$ is greater than $\tau$. Then, for each element left 
in $Q$, if it is a leaf or contains $k$ or fewer points, we remove it and all of its points to $Q$. Finally, we
replace all remaining non-leaf clusters in $Q$ with their children. 

We complete this process until $Q$ contains only points. At this point, we 
just use [INSERT ref to quicksort partition in paper] to find the $k$th 
nearest neighbor. Since this algorithm will also elements on the correct side of 
the index of the $k$th nearest neighbor, the first $k$ elements in $Q$ are the $k$-nearest neighbors.


\begin{algorithm} % enter the algorithm environment
    \caption{QuicksortPartition(\emph{Q, k, l, r})} % give the algorithm a caption
    \label{alg:quicksort-partition} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[5] % enter the algorithmic environment
        \IF{$l \geq r$}
            \STATE Return $min(l, r)$
        \ELSE 
            \STATE $pivot = l + (r - l) / 2$
            \STATE $Q.swap(pivot, r)$
            \STATE $a \Leftarrow l$
            \STATE $b \Leftarrow l$
            \WHILE {$b < r$}
                \IF {$Q[b].\delta_{max} < Q[r].\delta_{max}$}
                    \STATE $Q.swap(a, b)$
                    \STATE $a \Leftarrow a + 1$
                \ENDIF
                \STATE $b \Leftarrow b + 1$
            \ENDWHILE
            \STATE $Q.swap(a, r)$
            \STATE $p \Leftarrow r$
            \STATE $g \Leftarrow \sum_{i=0}^{p} |Q[i]|$
            \IF {$g = k$}
                \STATE Return $p$
            \ELSIF {$g < k$}
                \STATE $QuicksortPartition(Q, k, p + 1, r)$
            \ELSE 
                \IF {$p > 0 \land g > k + |Q[p - 1]|$}
                    \STATE $QuicksortPartition(Q, k, l, p - 1)$
                \ELSIF {$p > 0 \land g = k + |Q[p - 1]|$}
                    \STATE Return $p - 1$
                \ELSE
                    \STATE Return $p$
                \ENDIF
            \ENDIF
        \ENDIF
    \end{algorithmic}
    \end{algorithm}

\begin{algorithm} % enter the algorithm environment
    \caption{Sieve(\emph{tree, query, k})} % give the algorithm a caption
    \label{alg:sieve} % and a label for \ref{} commands later in the document
    \begin{algorithmic}[6] % enter the algorithmic environment
        \REQUIRE $tree$, $query$, $k$
        \STATE $Q \Leftarrow$ [$tree.root$]
        \WHILE{$|Q| > k$}
            \STATE $i \Leftarrow QuicksortPartition(Q, k, 0, |Q| - 1)$
            \STATE $\tau \Leftarrow Q[i].\delta_{max}$
            \FOR {$q \in Q$}
                \IF {$q.\delta_{min} > \tau$}
                    \STATE $Q.pop(q)$
                \ENDIF
            \ENDFOR
            \FOR {$q \in Q$}
                \IF {$q.isLeaf \lor |q| \leq k$}
                    \STATE $Q.push(q.points)$
                \ELSE
                    \STATE $[l, r] \Leftarrow q.children$
                    \STATE $Q.push([l, r])$   
                \ENDIF
                \STATE $Q.pop(q)$
            \ENDFOR 
        \ENDWHILE
        \STATE Return $Q$
    \end{algorithmic}
    \end{algorithm}

\subsubsection{Sieve Search with Separate Centers}
\label{subsubsec:methods:knn-search:sieve2}
Sieve Search with Separate Centers is the same as Sieve Search, but with the following modification: clusters 
in $Q$ are represented twice-- once as their center and once as the rest of their points. 
Because we have that for any cluster $C$, $C.\delta \leq C.\delta_{max}$, representing the cluster 
center separately from the rest of the points causes the threshold $\tau$ to decrease more quickly, 
thus allowing us to eliminate some clusters from $Q$ earlier in the process.


\subsubsection{Greedy Sieve}
\label{subsubsec:methods:knn-search:greedy-search}

With Greedy Sieve, we first let $Q$ be a min queue of clusters by non-increasing $\delta_{min}$. We initialize $Q$ with the root cluster.
Let $H$ be a fixed-size ($k$) max queue of points by $\delta$. This queue starts empty.

While $H$ has fewer than $k$ clusters or $\delta$ of the top priority element in $H$ is greater 
than $\delta_{min}$ of the top priority element in $Q$, do the following:
\begin{enumerate}
\item While the top priority element $q$ in $Q$ is not a leaf, pop $q$ from the queue and push its children.
\item For each point $p \in q$, push $p$ to $H$. 
\item Pop points from $H$ until $H$ has $k$ points. 
\end{enumerate}
After this process, the points left in $H$ are the $k$ nearest neighbors of $q$.

\begin{algorithm} 
\caption{GreedySearch(\emph{root, query, k})} 
\label{alg:greedy_search} 
\begin{algorithmic}[3]
    \STATE $Q \Leftarrow$ priority queue
    \STATE $Q.push(root)$
    \STATE $H \Leftarrow$ priority queue
    \WHILE{$|H| < k \lor H.pop.\delta > Q.pop.\delta_{min}$}
        \WHILE{$\neg Q.pop.isLeaf$}
            \STATE $[l, r] \Leftarrow Q.pop().children$
            \STATE $Q.push([l, r])$
        \ENDWHILE
        \STATE $leaf \Leftarrow Q.pop()$
        \STATE $H.push(leaf)$
        \WHILE{$|H| > k$}
            \STATE $H.pop()$
        \ENDWHILE
    \ENDWHILE
    \STATE Return $H$
\end{algorithmic}
\end{algorithm}

\subsubsection{Complexity of $k$-NN Search}
\label{paragraph:methods:knn-complexity}

To bound the complexity of $k$-NN search, we group all of the "Sieve" algorithms (Sieve, 
Sieve with Separate Centers, and Greedy Sieve) together, and handle $k$-NN by Repeated $\rho$-NN separately.

We start by considering the complexity of $k$-NN by Repeated $\rho$-NN, as this  
is a natural extension of the complexity bounds for $\rho$-NN search described in 
\cite{ishaq2019clustered}. For this algorithm, we adopt the terminology used in 
\cite{ishaq2019clustered} and \cite{yu2015entropy} and address \emph{coarse search} and \emph{fine search} separately. 
Coarse search refers to the process of identifying clusters
which have overlap with the query ball (i.e., which are likely to contain one of the $k$ nearest neighbors). 
Fine search refers to the process
of identifying the $k$ nearest neighbors among the points in those clusters identified by coarse search.




% Our $k$-NN algorithms do not use two completely distinct phases of coarse and fine search, 
% as is the case in $\rho$-NN search. Still, this distinction is useful in analyzing 
% the asymptotic complexity of $k$-NN search. In particular, we can think of the complexity of 
% coarse search as an estimate of how many clusters we examine during search, and 
% the complexity of fine search as an estimate of how many clusters we must examine all the points 
% from during search, even if we do not actually perform these as two distinct phases. 


% Our complexity bounds depend on the same technical assumption about the self-similarity of the dataset made in 
% \cite{yu2015entropy}: if call the number of points within a given 
% radius of a point the \emph{density} around that point, we assume that for this radius, the densities around all points
% are bounded within a constant multiplicative factor $\gamma$ of each other. Formally, 
% we assume that for any point $p$, query $q$, and radius 
% $\rho$, \begin{equation} \frac{1}{\gamma}|B(p, \rho)| \leq \mathbb{E}[|B(q, \rho)|] \leq \gamma |B(p, \rho)|. \label{3} \end{equation}

% For tree search, in the worst case, we must consider [lift ideas here from chess for this justification and ask 
% if this still holds for knn]. 

% To determine the asymptotic complexity of fine search, we must estimate $|F|$, the 
% number of clusters we must examine all the points from during search. With $\rho$-NN 
% search, $F$ is the union of clusters which are within a distance of $r + r_c$ from the query,
% where $r$ is the search radius and $r_c$ is the radius of leaf clusters,
% as in \cite{yu2015entropy}. With $k$-NN search, however, 
% the value of $r$ is dependent on the distance to the $k$th nearest neighbor, which 
% is query-dependent. Thus, for $k$-NN search, we  adjust the definition of 
% $F$ to be the union of clusters which are within a distance of $r' + r_c$ from the query, 
% where $r'$ is the distance to the $k$th nearest neighbor and $r_c$ is defined as before.

% By the triangle inequality, we have that $F \subseteq B(q, r' + 2r_c)$, 
% where $B(q, r' + r_c)$ is the ball of radius $r' + r_c$ centered at $q$.
% Hence, $|F| \leq |B(q, r' + 2r_c)|$.

% Now we have that \begin{align*} \mathbb{E}_q[|B(q, r' + 2r_c)|] &\leq \gamma|B(p, r' + 2r_c)| \\ %because of the assumption that density at different locations on the manifold differ by only a constant factor%
%     &\leq \gamma|B(p, r')|\left(\frac{r' + 2r_c}{r'}\right)^d \label{5}\\ %because d = lfd tells us how the number of additional hits reached scales with a change in radius, and our ratio between radii is (r' + 2r_c)/r'%
%     &\leq \gamma^2 \mathbb{E}[|B(q, r')|]\left(\frac{r'+2r_c}{r'}\right)^d \\ %using density bound again to go from a statement about an arbitrary point back to a statement about the query%
%     &\leq \gamma^2 k\left(\frac{r'+ 2r_c}{r'}\right)^d  \\   %using the fact that for k-nn, the number points in the query ball is k%
% \end{align*}

% It remains to provide an estimate for $r'$. As described in \cite{yu2015entropy} bounded density assumption suggests that increase in number of
% points reached by doubling the radius has to be roughly uniform across all regions of the manifold. Thus, the global 
% average local fractal dimension over all regions of the dataset is not too different from the local fractal dimension around any particular point.
% We rely on this fact in our estimation of $r'$. 

% We let $d'$ be the average of the local fractal dimension across all clusters in the dataset and based on the bounded density assumption, we have that
% $d' \approx d_{0}$, where $d_0$ is the local fractal dimension of the root cluster (i.e., $d_0$ is the logarithm of the ratio between the cardinality of the root cluster 
% and the cardinality of the ball centered at the root clustIn this algorithm, we perform $\rho$-nearest neighbors search at equal to the radius of the cluster tree divided by
% er's center with half its radius). While, ordinarily we compute local fractal dimension 
% by comparing cardinalities of balls centered at some point at two different radii, in order to estimate $r'$, instead compare the cardinality of 
% a ball \emph{around the query} of radius $r'$ to the cardinality \emph{of the root cluster} at its radius.

% If $C_0$ is the root cluster, $r_0$ is its radius, then we have 
% \begin{equation} d_0 = \frac{\log{}\frac{|C_0|}{k}}{\log{}\frac{r_0}{r'}}. \label{4} \end{equation}

% Since $d_0$, $|C_0|$, $r_0$, and $k$ are all known values, we can solve equation 4 for $r'$ to get
% \begin{equation} r' = r_0\left(\frac{k}{|C_0|}\right)^{\frac{1}{d_0}}. \label{6} \end{equation}
