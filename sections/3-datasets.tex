\section{Datasets And Benchmarks}
\label{sec:datasets-and-benchmarks}

\subsection{ANN-Benchmark Datasets}
\label{sec:datasets-and-benchmarks:ann-benchmark-datasets}

We benchmark on a variety of datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
Table~\ref{tab:datasets:summary} summarizes these datasets.
All benchmarks were conducted on an Intel Xeon E5-2690 v4 CPU @ 2.60GHz with 512GB RAM.
The OS kernel was Manjaro Linux 5.15.164-1-MANJARO.
The Rust compiler was Rust 1.83.0, and the Python interpreter version was 3.9.18.

\begin{table}
    \caption{Datasets used in benchmarks.}
    \label{tab:datasets:summary}
    \begin{center}
        \begin{sc}
            \begin{tabular}{|l|l|l|l|}
                \hline
                \textbf{Dataset} & \textbf{Distance Function}  &\textbf{Cardinality}  & \textbf{Dimensionality}  \\
                \hline
                Fashion-Mnist    & Euclidean                   & 60,000             & 784                    \\
                \hline
                Glove-25         & Cosine                      & 1,183,514          & 25                     \\
                \hline
                Sift             & Euclidean                   & 1,000,000          & 128                    \\
                \hline
                Random           & Euclidean                   & 1,000,000          & 128                    \\
                \hline
                SILVA            & Levenshtein                 & 2,224,640          & 3,712 - 50,000         \\
                \hline
                RadioML          & Dynamic Time Warping        & 97,920             & 1,024                  \\
                \hline
            \end{tabular}
        \end{sc}
    \end{center}
    \vskip -0.1in
\end{table}

\subsection{Random Datasets and Synthetic Augmentation}
\label{sec:datasets-and-benchmarks:random-datasets}

In addition to benchmarks on datasets from the ANN-Benchmarks suite, we also benchmarked on synthetic augmentations of these real datasets, using the process described in \ref{sec:methods:synthetic-data}.
In particular, we use a noise tolerance $\epsilon = 0.01$ and explore the scaling behavior as the cardinality multiplier (referred to as ``Mult.'' in Tables~\ref{tab:results:qps-and-recall-fmn},~\ref{tab:results:qps-and-recall-glove},~\ref{tab:results:qps-and-recall-sift} and~\ref{tab:results:qps-and-recall-random}) increases.
We also benchmarked on purely randomly-generated datasets of various cardinalities.
For this, we used a base cardinality of 1,000,000 and a dimensionality of 128 to match the Sift dataset; hereafter, we refer to the random dataset with cardinality 1,000,000 and dimensionality 128 as ``Random.''
This benchmark allows us to isolate the effect of a manifold structure (which we expect to be absent in a purely random dataset) on the performance of the CAKES' algorithms.

In order to calculate recall on the augmented datasets, we perform linear search in Rust and save the results to disk.
We verified that linear search on the original dataset produced neighbors with perfect recall compared to those provided by the ANN-Benchmarks suite.
We then use the results of linear search on the augmented datasets as ground truth for calculating recall of CAKES's algorithms in Rust and read them in Python for calculating recall of HNSW, ANNOY, and FAISS-IVF.


\subsection{SILVA 18S}
\label{sec:datasets-and-benchmarks:silva-18s}

To demonstrate CAKES with a more exotic distance function, we also benchmarked on the SILVA 18S ribosomal RNA dataset~\cite{10.1093/nar/gks1219}.
This dataset contains ribosomal RNA sequences of 2,224,640 genomes with an aligned length of 50,000 letters.
We held out a set of 1,000 random sequences from the dataset to use as queries for benchmarking.
We use Levenshtein~\cite{levenshtein1966binary} distance on the unaligned sequences to build the tree and to perform $k$-NN search.
Since this dataset contains many redundant, i.e.,\,nearly identical, sequences, we used a maximum tree-depth of 128 as a partition criterion in Algorithm~\ref{alg:methods:partition}.


\subsection{Radio ML}
\label{sec:datasets-and-benchmarks:radio-ml}

As another example of an exotic distance function, we benchmarked CAKES on the RadioML dataset~\cite{oshea2018radioml}.
The RadioML dataset contains samples of synthetically generated signal captures of different modulation modes over a range of SNR levels.
Specifically, it comprises 24 modulation modes at 26 different signal-to-noise ratio (SNR) levels ranging from -20 dB to 30 dB, with 4,096 samples at each combination of modulation mode and SNR level.
Thus, it contains $24 \cdot 26 \cdot 4096 = 2,555,504$ samples in total.
Each sample is a 1,024-dimensional complex-valued vector, representing a signal capture (a time-series of complex-valued numbers).
We used a subset of this dataset, containing $97,304$ samples at 10dB SNR and used another $1,000$ samples at the same SNR as a hold-out set of queries.
We use Dynamic Time Warping~\cite{muller2007dynamic} as the distance function for this dataset.


\subsection{Other Algorithms}
\label{sec:datasets-and-benchmarks:other-algorithms}

We benchmarked CAKES's algorithms against the state-of-the-art similarity search algorithms HNSW, ANNOY and FAISS-IVF, and two implementations of linear search.
In particular, we use FAISS-Flat, a Python implementation of linear search, to provide ground-truth values for recall of HNSW, ANNOY, and FAISS-IVF.
We use our own Rust implementation of linear search to provide ground-truth values for calculating recall of CAKES's algorithms.
We plot the throughput of these two implementations of linear search as a reference point for all other algorithms in Figure~\ref{fig:results:scaling-plots}.
