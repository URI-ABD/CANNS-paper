\section{Discussion and Future Work}
\label{sec:discussion}

We have presented CAKES, a set of three algorithms for fast $k$-nearest-neighbors search that can adapt to a variety of distance functions.
CAKES's algorithms are exact when the distance function in use is a metric (particularly, when it obeys the triangle inequality). When we benchmarked the algorithms with cosine distance, a non-metric, we found that CAKES has \emph{nearly} perfect recall; at three decimal places of precision, CAKES's algorithms have recall indistinguishable from perfect. 


CAKES's algorithms were designed to be most effective when the dataset is not uniformly distributed, but rather adheres to a low-dimensional manifold even when embedded in a high-dimensional space; 
in other words, CAKES was designed to work best when the manifold hypothesis is upheld.Our results indicate that CAKES algorithms do indeed perform well on datasets that adhere to the manifold hypothesis, and that they perform poorly on datasets that do not. 
We use the LFD by depth percentiles shown in Section~\ref{subsec:lfd-results} to quantify the extent to which a dataset obeys the manifold hypothesis; we say that
the lower a dataset's LFD, the more that dataset exhibits a manifold structure. We observe a striking example of this difference when contrasting CAKES's algorithms' performance on Sift versus on the random dataset with the same cardinality and dimensionality as Sift. 


As seen in Section~\ref{subsec:lfd-results}, the local fractal dimension of the random dataset is much higher than that of Sift.
This is unsurprising, given that the random dataset is, by definition, uniformly distributed, while Sift is not.
On the random dataset, CAKES's algorithms' throughputs decrease as the cardinality multiplier increases, while with Sift, GreedySieve and Sieve both exhibit nearly constant throughput.
While Repeated $\rho$-NN does not exhibit constant throughput with Sift, it does exhibit a much slower decrease in throughput than it does with the random dataset.
Since these two datasets are the same size, the aforementioned discrepancies isolate the effect of the manifold structure of the dataset on algorithm performance.
We emphasize that though CAKES's algorithms' throughputs are lower on a random dataset, their recalls remain perfect at all multipliers for both datasets. 
A sort of converse of this trend occurs with HNSW and Annoy. 
In contrast, for these two algorithms, \emph{throughput} remains constant with cardinality multiplier on both datasets, but their \emph{recall} for the random dataset is orders of magnitude worse than their recall for Sift.
These results illustrate the difference between CAKES's algorithms and the state-of-the-art similarity search algorithms. They also demonstrate CAKES's algorithms suitability on the type of datasets for which they were designed.


We also note that on different datasets which exhibit manifold structures, the highest-throughput CAKES algorithm varies. 
For example, on Glove-25, Repeated $\rho$-NN exhibits the highest throughput of any CAKES algorithm, despite having the lowest throughput of the three on Sift and Fashion-mnist. 
On Silva,  Repeated $\rho$-NN is comparable to the other CAKES algorithms at low multipliers, but becomes the best of the three at higher multipliers. 
When we view these results in light of the LFD plots in Section~\ref{subsec:lfd-results}, we realize that  Repeated $\rho$-NN appears to be the best-performing CAKES algorithm on datasets where a large proportion of the dataset has a very low LFD (i.e., near 2), like Glove-25 and Silva. 
This general trend is unsurprising, given that Repeated $\rho$-NN relies on a low LFD around the query in order to quickly home in on the correct radius for $k$ hits. 
While it varies which of the two algorithms has better throughput, we observe that on all three ANN benchmark datasets (Fashion-mnist, Glove-25, and Sift), GreedySieve's and Sieve's throughput stays nearly constant as the cardinality multiplier increases. 
This observation warrants further investigation, but it is especially promising given that GreedySieve and Sieve consistently outperform linear search for high cardinalities. 
Finally, we emphasize that the variation in performance of our algorithms across different datasets and cardinalities support our use of an auto-tuning function (as discussed in Section~ref{subsec:methods:auto-tuning}) to select the best choice of algorithm for a given dataset. 
Given the trends such as that with low LFD and Repeated $\rho$-NN's performance, we suspect that a more sophisticated, less-heuristic auto-tuning function could be developed to select the best algorithm based on the properties of the dataset.


We also stress that the CAKES's algorithms are designed to work well for \emph{big} data, and our results support this claim; we observe that while linear search can outperform CAKES on datasets with small cardinality, CAKES always overtakes linear search at some cardinality multiplier for each of the ANN benchmark datasets we tested on. Moreover, we notice that the cardinality multiplier at which CAKES overtakes linear search is lower for datasets with lower LFD. For example, Sift has much higher LFD than Fashion-mnist and Glove-25, and we observe that CAKES overtakes linear search at a cardinality of $10^7$ for Sift, as opposed to about $10^5$ and $10^6$ for Fashion-mnist and Glove-25, respectively. For the truly random dataset, which had the highest LFD, CAKES's algorithms \emph{never} outperform linear search. These observations support our claims that CAKES's algorithms work well with big datasets, and further support our claims that they are much better suited to datasets which exhibit a manifold structure.


% Is this still true: "However, the ann-benchmarks results show that even when FAISS and HNSW are tuned for lower recall, they are still slower than CAKES in all cases."

The questions raised by this study suggest several avenues for future work.
A comparison across more datasets and larger datasets is clearly in order, as is further analysis with distance functions that existing methods such as FAISS do not support, such as Wasserstein distance~\cite{vallender1974calculation} (particularly for arbitrary dimensionality), and Tanimoto distance~\cite{bajusz2015tanimoto}, which is particularly applicable to computing distances based on maximal common subgraphs of molecular structures.  % TODO: Add Lev, NW, SW
Incorporating these or other distance functions in CAKES only requires a Rust implementation of the distance function.


In future work, we plan to investigate compression by representing differences at each level of the binary tree, particularly for string or genomic data, where all differences are discrete.
When we descend some levels in the cluster tree, it takes fewer bits to encode each point in terms of the center because the radii decrease.
However, each descent of a level in the cluster tree increases the bit-cost of specifying a particular cluster. 
This presents the following tradeoff:
\emph{How do we algorithmically determine the depth to which we should cluster a dataset to achieve optimal compression?} 
In our future work, we will explore which properties of the dataset and of clusters can inform this optimal depth.

We would like to add the ability to perform ``online'' updates to the tree as points are added to or deleted from a dataset.
This would enable CAKES to be used in a streaming environment.

We also plan to use CAKES with anomaly detection.
Our anomaly detection tool, CHAODA~\cite{ishaq2021clustered}, uses a variety of methods in an ensemble to determine an anomaly score for a point.
We would like to explore the effectiveness of using the distribution of distances among the $k$-th nearest neighbors as an additional method in our ensemble.

CLAM and CAKES are implemented in Rust and their source code is available under an MIT license at https://github.com/URI-ABD/clam.

% since this online update takes advantage of CAKES' fast search, it allows us to have our cake and eat it, too.