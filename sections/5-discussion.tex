\section{Discussion}
\label{sec:discussion}

Discuss results and future work \dots

CLAM-CAKES.
This is most effective when datasets exhibit low metric entropy and low local fractal dimension.

Unlike LSH, this is extensible to any user provided distance function.

Exactness of search and metric vs non-metric.

Is it the case that the optimal clusters for compression are also the optimal clusters for accelerated search?

What kind of trade-offs can we offer for space-savings of compression vs time-savings of accelerated-search in the compressed space?

\subsection{Future Works}
\label{subsec:results:future-works}

GPU-acceleration for the more computationally intensive distance functions like wasserstein.

Live-updates as new points are added to the dataset.
For example, build tree with a subsample of the full data and then add the rest of the instances to simulate live-updates.

More distance functions. For example, Tanimoto Distance using maximal-common-subgraph for molecular structures.

Better methods for constructing minimal encodings, perhaps using domain expertise.
