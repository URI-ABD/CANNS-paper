\section{Discussion}
\label{sec:discussion}

Discuss results and future work \dots

CLAM-CAKES.
This is most effective when datasets exhibit low metric entropy and low local fractal dimension.

Unlike LSH, this is extensible to any user provided distance function.

Exactness of search and metric vs non-metric.

While CAKES has perfect recall for distance metrics, it exhibits worse recall under non-metrics (specifically, cosine distance). This bears further investigation, but likely stems from how heavily the knn and clustering algorithms depend on the triangle inequality.

\subsection{Future Works}
\label{subsec:results:future-works}

For future work, we plan to investigate compression by representing differences at each level of the
binary tree, particularly for string or genomic data, where all differences are discrete.
When we descend a level in the cluster tree, it takes fewer bits to encode each point in terms of the center. 
However, each descent of a level in the cluster tree increases the bit-cost of specifying a particular cluster. 
This presents the following tradeoff: \emph{How do we algorithmically determine the depth to which we should cluster a dataset to achieve optimal compression?} 
In our future work, we will explore which properties of the dataset and of clusters can inform this optimal depth. 


We also plan to use CAKES with anomaly detection. Our anomaly detection tool, CHAODA~\cite{ishaq2021clustered}, uses 
a variety of methods in an ensemble to determine an anomaly score for a point. We would like to explore the effectiveness of 
distance to the $k$-th nearest neighbor as an additional method in our ensemble.


Additionally, we would like to implement additional distance functions to improve the genericity of CAKES. 
For example, we would like to implement Tanimoto Distance using maximal-common-subgraph for molecular structures. Ultimately, 
we would also like to implement Wasserstein distance for dimensions higher than two. In order to make this possible, 
we plan to implement GPU-acceleration for the more computationally-intensive distance functions like Wasserstein.

Finally, we would like to add the ability to view live-updates as new points are added to a dataset. For example, 
we could build the cluster tree with a subsample of the full data set and then add the rest of the instances to 
simulate live updates.

