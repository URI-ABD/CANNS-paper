\section{Discussion and Future Work}
\label{sec:discussion}

We have presented CAKES, a set of three algorithms for fast $k$-nearest-neighbors search that can adapt to a variety of distance functions.
CAKES is an exact $k$-NN search algorithm when the distance function in use is a metric (particularly, when it obeys the triangle inequality).
{\color{red} TODO: Mention (probably earlier than here) that we modified cosine so that it satisfies the triangle inequality and now have perfect recall on it}
CAKES algorithms were designed to be most effective when the dataset is not uniformly distributed, but rather adheres to a low-dimensional manifold even when embedded in a high-dimensional space; 
in other words, CAKES was designed to work best when the manifold hypothesis is upheld.

Our results indicate that CAKES algorithms do indeed perform well on datasets that adhere to the manifold hypothesis, and that they perform poorly on datasets that do not. 
We use plots of the local fractal dimension of the datasets to quantify the extent to which a dataset exhibts a manifold structure; 
the lower the local fractal dimension across the entire dataset, the more the dataset adheres to the manifold hypothesis.


The results of our benchmarks on sift and on the random dataset with the same cardinality and dimensionality as sift illustrate a striking example of this difference.
First, we note that, as discussed in Section~\ref{subsec:lfd-results}, the local fractal dimension of the random dataset is much higher than that of sift.
This is unsurprising, given that the random dataset is, by definition, uniformly distributed, while sift is not.
On the random dataset, the CAKES algorithms' throughputs decrease as the cardinality multiplier increases, with sift, GreedySieve and Sieve both exhibit nearly constant throughput as the cardinality multiplier increases.
While RepeatedRnn does not exhibit constant throughput with sift, it does exhibit a much slower decrease in throughput than it does with the random dataset.
Since these two datasets are the same size, the discrepancy in algorithm performance on each one isolates the effect of the manifold structure of the dataset on algorithm performance.
We emphasize that though CAKES algorithms' throughputs are lower on a random dataset, their recalls remain perfect at all multipliers for both datasets. 
The reverse of this trend occurs with HNSW and Annoy. 
In contrast, for these two algorithms, throughput remains constant with cardinality mutliplier on both datasets, but their recall on the random dataset is orders of mangnitude worse than their recall on sift.
These results illustrate the difference between CAKES algorithms and the state-of-the-art, as well as their suitability for the datasets for which they were designed.


We also note that on different datasets which exhibit manifold structures, the best CAKES algorithm varies. 
For example, on Glove-25, RepeatedRnn is the best-performing CAKES algorithm, despite being the worst-perfomring of the three on sift and fashion-mnist. 
On Silva, RepeatedRnn is comparable to the other CAKES algorithms at low multipliers, but becomes the best of the three at higher multipliers. 
When we view these results in light of the local fractal dimension plots in Section~\ref{subsec:lfd-results}, we realize that RepeatedRnn is the best-performing CAKES algorithm on datasets where a large proportion of the dataset has a very low (near 2) local fractal dimension, like Glove-25 and Silva. 
This general trend is unsurprising, given that Repeated $\rho$-NN relies on a low local fractal dimension around the query in order to quickly home in on the correct radius for $k$ hits. 
While it varies which of the two algorithms has better throughput, we observe that on all three ANN benchmark datasets (fashion-mnist, glove-25, and sift), GreedySieve's and Sieve's throughput stays nearly constant as the cardinality multiplier increases. 
This observation warrants further investigation, but it is especially promising given that GreedySieve and Sieve consistently outperform linear search for high cardinalities. 
Finally, we emphasize that the variation in performance of our algorithms across different datasets and cardinalities support our use of an autotuning function to select the best choice of algorithm for a given dataset. 
Given the trends such as that with low local fractal dimension and RepeatedRnn's performance, we suspect that a more sophisticated, less-heuristic autotuning function could be developed to select the best algorithm based on the properties of the dataset.

We also stress that the CAKES algorithms are designed to work well for \emph{big} data, and our results support this claim; we observe that while linear search can outperform CAKES on datasets with small cardinality, CAKES always overtakes linear search at some cardinality multuplier when the dataset exhibits a manifold structure.

{\color{red} TODO: be consistent with naming (probably don't call it RepeatedRnn in text and call it Repeated $\rho$-NN instead); will deal with this later} 


{\color{red} TODO: what happened with the pivot point for Sieve? Or is it just less prominent in these plots? }
% We also notice an interesting trend with Sieve and Sieve with Separate centers: for all datasets, at both values of $k$, both algorithms exhibit a sharp ``pivot point'' after which throughput starts to increase. 
% While we observe that this pivot point seems to be proportional to the value of $k$, as it often occurs at about a multiplier of 4 when $k=10$ and a mulitplier of 32 when $k=100$, the reason for this behavior requires further investigation.  



% Is this still true: "However, the ann-benchmarks results show that even when FAISS and HNSW are tuned for lower recall, they are still slower than CAKES in all cases."

The questions raised by this study suggest several avenues for future work.
A comparison across more datasets and larger datasets is clearly in order, as is further analysis with distance functions that existing methods such as FAISS do not support, such as Wasserstein distance~\cite{vallender1974calculation} (particularly for arbitrary dimensionality), and Tanimoto distance~\cite{bajusz2015tanimoto}, which is particularly applicable to computing distances based on maximal common subgraphs of molecular structures.  % TODO: Add Lev, NW, SW
Incorporating these or other distance functions in CAKES only requires a Rust implementation of the distance function.


In future work, we plan to investigate compression by representing differences at each level of the binary tree, particularly for string or genomic data, where all differences are discrete.
When we descend some levels in the cluster tree, it takes fewer bits to encode each point in terms of the center because the radii decrease.
However, each descent of a level in the cluster tree increases the bit-cost of specifying a particular cluster. 
This presents the following tradeoff:
\emph{How do we algorithmically determine the depth to which we should cluster a dataset to achieve optimal compression?} 
In our future work, we will explore which properties of the dataset and of clusters can inform this optimal depth.

We would like to add the ability to perform ``online'' updates to the tree as points are added to or deleted from a dataset.
This would enable CAKES to be used in a streaming environment.

We also plan to use CAKES with anomaly detection.
Our anomaly detection tool, CHAODA~\cite{ishaq2021clustered}, uses a variety of methods in an ensemble to determine an anomaly score for a point.
We would like to explore the effectiveness of using the distribution of distances among the $k$-th nearest neighbors as an additional method in our ensemble.

CLAM and CAKES are implemented in Rust and their source code is available under an MIT license at https://github.com/URI-ABD/clam.

% since this online update takes advantage of CAKES' fast search, it allows us to have our cake and eat it, too.