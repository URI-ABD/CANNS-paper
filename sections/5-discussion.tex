\section{Discussion and Future Work}
\label{sec:discussion}

{\color{red} TODO: Use lfd plots to discuss repeated rnn being best for glove low lfd.}

% Moving stuff out of results into discussion: 
These results illustrate the fact that \emph{ceteris paribus}, absent a manifold structure in the dataset, CAKES's algorithms do not begin to outperform linear search until the cardinality multiplier is around 10. 
Repeated $\rho$-NN shows significantly worse performance on this dataset relative to both the other algorithms and to its own performance on real datasets. 
This is unsurprising, given that Repeated $\rho$-NN relies on a low local fractal dimension around the query in order to quickly home in on the correct radius for $k$ hits. 
With a completely random dataset, whose average local fractal dimension will be near its embedding dimension, Repeated $\rho$-NN will iterate for longer before finding this correct radius. 

Notably, we observe that on all datasets, GreedySieve's throughput stays nearly constant as cardinality increases. This observation warrants further investigation, but it is especially promising given that GreedySieve consistently outperforms linear search for high cardinalities. 
% We also notice an interesting trend with Sieve and Sieve with Separate centers: for all datasets, at both values of $k$, both algorithms exhibit a sharp ``pivot point'' after which throughput starts to increase. 
% While we observe that this pivot point seems to be proportional to the value of $k$, as it often occurs at about a multiplier of 4 when $k=10$ and a mulitplier of 32 when $k=100$, the reason for this behavior requires further investigation.  
Finally, we reiterate that the variation in performance of our algorithms across different datasets and cardinalities support our use of an autotuning function to select the best choice of algorithm. 
% Currently, our autotuning function always uses $k=10$ for its determination, but our observation that the best algorithm often differs between small and large values of $k$ suggests that more sophisticated autotuning is necessary.




We have presented CAKES, a method for fast $k$-nearest-neighbors search that can adapt to a variety of distance functions.
CAKES provides exact recall with $k$-NN when the distance function in use is a metric (particularly, when it obeys the triangle inequality).
CAKES is most effective when datasets are not uniform and do not obey a ``simple'' distribution such as Gaussian, but rather adhere to ``interesting'' low-dimensional manifolds even when embedded in a high-dimensional space;
thus, CAKES works best when the manifold hypothesis is upheld.

In comparison to other methods, while CAKES has perfect recall for distance metrics, it exhibits worse recall under non-metrics (specifically, cosine distance). This bears further investigation, but likely stems from how heavily the $k$-NN and clustering algorithms depend on the triangle inequality when discarding clusters from consideration.

% Is this still true: "However, the ann-benchmarks results show that even when FAISS and HNSW are tuned for lower recall, they are still slower than CAKES in all cases."

The questions raised by this study suggest several avenues for future work.
A comparison across more datasets and larger datasets is clearly in order, as is further analysis with distance functions that existing methods such as FAISS do not support, such as Wasserstein distance~\cite{vallender1974calculation} (particularly for arbitrary dimensionality), and Tanimoto distance~\cite{bajusz2015tanimoto}, which is particularly applicable to computing distances based on maximal common subgraphs of molecular structures.  % TODO: Add Lev, NW, SW
Incorporating these or other distance functions in CAKES only requires a Rust implementation of the distance function.


% For all datasets benchmarked, the auto-tuning method selected repeated $\rho$-nearest neighbor (Section~\ref{subsubsec:methods:knn-search:repeated-rnn}).
% However, during limited testing on synthetic datasets obeying simple distributions, this was not always the case.
% We seek to evaluate these approaches on many more datasets and distance metrics;
% it could be that repeated $\rho$-NN is always best (and thus the auto-tuning is unnecessary), but such a determination would be premature.

In future work, we plan to investigate compression by representing differences at each level of the binary tree, particularly for string or genomic data, where all differences are discrete.
When we descend some levels in the cluster tree, it takes fewer bits to encode each point in terms of the center because the radii decrease.
However, each descent of a level in the cluster tree increases the bit-cost of specifying a particular cluster. 
This presents the following tradeoff:
\emph{How do we algorithmically determine the depth to which we should cluster a dataset to achieve optimal compression?} 
In our future work, we will explore which properties of the dataset and of clusters can inform this optimal depth.

We would like to add the ability to perform ``online'' updates to the tree as points are added to or deleted from a dataset.
This would enable CAKES to be used in a streaming environment.

We also plan to use CAKES with anomaly detection.
Our anomaly detection tool, CHAODA~\cite{ishaq2021clustered}, uses a variety of methods in an ensemble to determine an anomaly score for a point.
We would like to explore the effectiveness of using the distribution of distances among the $k$-th nearest neighbors as an additional method in our ensemble.

CLAM and CAKES are implemented in Rust and their source code is available under an MIT license at https://github.com/URI-ABD/clam.

% since this online update takes advantage of CAKES' fast search, it allows us to have our cake and eat it, too.