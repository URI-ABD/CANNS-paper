\section{Information Theoretic Analysis}
\label{sec:information-theoretic-analysis}

We extend the work from~\cite{berger2020levenshtein} for search and compression using a flat clustering to our case of a hierarchical clustering.
We further generalize this for any dataset and distance function described below.
We also provide relevant algorithms for some pairings of datasets and distance functions in Section~\ref{subsec:methods:compression}.

We take a \textbf{dataset} $X$ with $n$ instances embedded in a $\mathcal{D}$-dimensional Banach space defined by a distance function $f: X \times X \mapsto \mathbb{R}$.
We assume that the dataset follows the Manifold Hypothesis~\cite{fefferman2016testing}, i.e. instances in the dataset mostly lie on a $d$-dimensional manifold where $d \ll \mathcal{D}$.
We also make some uniformity assumptions for the dataset;
specifically that $|X|$ is finite and the minimum and maximum sampling densities in the dataset differ by some constant multiplicative factor.

The \textbf{distance function} must be a metric; that is it must have the properties that $\forall x, y, z \in X$:
\begin{enumerate}[i.]
    \item $f$ is deterministic.
    \item $f(x, y) = 0 \Leftrightarrow x = y$ and $f(x, y) > 0 \Leftrightarrow x \neq y$.
    \item $f(x, y) = f(y, x)$, i.e. symmetry.
    \item $f(x, z) \leq f(x, y) + f(y, z)$, i.e. the triangle inequality.
\end{enumerate}

% TODO: Add note about the triangle inequality.
%  Not always needed, without it, search might miss some hits.

We define a \textbf{Metric Ball} as $B(q, r) = \{ x \in X \ | \ f(q, x) \leq r \}$, i.e. a ball containing all instances in the dataset within distance $r$ from an instance $q$.
We also define a \textbf{Cluster} as $C(c, r)$ analogous to a Metric Ball with the caveat that if some clusters have overlapping volumes then any instances in that overlapping volume are assigned to only one of the clusters.
Thus $\bigcup C_i = X$ and $i \neq j \Rightarrow C_i \cap C_j = \phi$.

The \textbf{Metric Entropy} (Kolmogorov Entropy) of the dataset, denoted by $N_{\epsilon}^{ent}(X)$, is the maximum $n$ such that $\{x_1, \dots, x_n\} \subseteq X$ and $f(x_i, x_j) \geq \epsilon \ \forall \ i \neq j$, i.e. the maximum number of $\epsilon$-separated instances in the dataset.

The \textbf{Internal Covering Number} of the dataset, denoted by $N_{\epsilon}^{int}(X)$, is the minimum $n$ such that $\{x_1, \dots, x_n\} \subset X$ and $\bigcup_{i = 1}^{n} C(c_i, \epsilon) = X$, i.e. the minimum number of $\epsilon$-radius clusters that collectively cover $X$.

As in~\cite{berger2020levenshtein}, $N_{2\epsilon}^{ent}(X) \leq N_{\epsilon}^{int}(X) \leq N_{\epsilon}^{ent}(X)$, i.e. the Metric Entropy bounds the Internal Covering Number.
Thus, they are equivalent measures.

Given an \textbf{Internal Covering} of $X$, we can specify any $\bar{x} \in X$ to within error $\epsilon$ by specifying $x_i$ such that $\bar{x} \in B(x_i, \epsilon)$.
Thus it takes $\mathcal{O} \big( \log N_{\epsilon}^{ent}(X) \big)$ bits of information to specify $\bar{x}$ to within error $\epsilon$.

Given two instances $x_1, x_2 \in X$, we can construct $x_2 - x_1$, a \textbf{minimal encoding} of $x_2$ in terms of $x_1$, such that the number of bits needed to specify the encoding scales linearly with $f(x_1, x_2)$.
We would need a potentially different method for constructing such \textit{minimal encodings} for each pairing of $(X, f)$.
See Sections~\ref{subsec:methods:compression} and~\ref{sec:datasets-and-distance-functions} for some examples.

The \textbf{Fractal Dimension} (Minkowski Dimension) of a dataset is
\begin{align*}
    d(X) &= \lim_{\epsilon \rightarrow 0} \frac{\log \big( N_{\epsilon}^{ent}(X) \big) }{\log \big( \frac{1}{\epsilon} \big)}
\end{align*}
However, if $|X|$ is finite then $d(X) = 0$.

To remedy this, we define the \textbf{Local Fractal Dimension}, as in~\cite{berger2020levenshtein}, for range $r$ and scale $s$ as
\begin{align*}
    d(X, r, s) &= \max_{x \in X} \ \log \frac{|B(x, r + s)| \ / \ |B(x, s)|}{(r + s) \ / \ s}
\end{align*}

For a cluster, $C(c, r)$, its local fractal dimension is equivalently defined as
\begin{align}
    \label{eq:local-fractal-dimension}
    d(C) &= \log_2 \ \frac{|C(c, r)|}{|C(c, \frac{r}{2})|}
\end{align}

The \textbf{Shannon Entropy} of an instance $x$, denoted by $H(x)$, is the number of bits needed to exactly specify $x$.

Given an encoding, $x_2 - x_1$, of an instance $x_2$ in terms of another instance $x_1$, let $H(x_2 - x_1)$ be the Shannon Entropy of the encoding, i.e. the number of bits needed to store the encoding.
Therefore,
\begin{align*}
    H \big( \{ x_1, x_2 \} \big) \leq H(x_1) + H(x_2 - x_1)
\end{align*}
where $\{ x_1, x_2 \}$ is the concatenation of $x_1$ and $x_2$.

For example, given a dataset of genomic strings with the Levenshtein edit distance as the metric, let $x_1$ and $x_2$ be two strings in the dataset.
Let $x_2 - x_1$ be a (possibly non-unique) shortest list of edits to convert $x_1$ into $x_2$.
The length of this edit list is equal to the Levenshtein edit distance i.e. $|x_2 - x_1| = f(x_1, x_2)$.
As in~\cite{berger2020levenshtein}, each such edit can be stored using $\mathcal{O}(\log m)$ bits where $m$ is the maximum length of any string in the dataset.
Thus, the total number of bits needed for each such encoding is $\mathcal{O} \big( f(x_1, x_2) \big)$.

Let $H(C)$ be the Shannon Entropy of the cluster $C(c, r)$.
Hence,
\begin{align*}
    H (C) &\leq H(c) + \sum_{x \in C} H(x - c) \\
    &\leq H(c) + |C| \cdot H(y - c) \\ \\
    \text{where} \ y &= \argmax_{x \in C} \ f(x, c) \text{, or equivalently} \ f(y, c) = r.
\end{align*}

For a \textbf{Hierarchical Clustering}, let the cluster $C$ have 
parent $C_{parent}$, 
children $C_{left}$ and $C_{right}$, 
center $c \in C$, 
radius $r \in \mathbb{R}^+$, 
and an instance $y \in C$ such that $f(y, c) = r$.

We have two choices for encoding the instances in a cluster:
either each instance can be encoded in terms of the cluster-center, 
or each child-cluster can be encoded in terms of the child-center and the child-centers encoded in terms of the cluster-center.

Thus the Shannon Entropy of a cluster encoding is given by
\begin{align}
    \label{eq:hierarchical-shannon-entropy}
    H(C) &\leq H(c - c_{parent}) + min \begin{cases}
        |C| \cdot H(y - c) \\
        H(C_{left}) + H(C_{right})
    \end{cases}
\end{align}

where the center of the root cluster is encoded directly rather than in terms of another instance.

% Noah
% Wait, can't we encode any cluster at depth d with at most d bits? For a binary tree, arbitrarily choose 0 for left and 1 for right, and a length-d traversal uniquely identifies a cluster at depth d. Not sure if this is a weaker upper bound, but we should mention it. For n flat clusters, lg n bits identifies a unique one; for n leaves, lg n bits identifies a leaf, but an internal cluster at depth d only needs lg d bits.
% Actually I see that you sort of get at this in the "back to shannon entropy" subsection, but maybe it could be explained intuitively as I did above?

% Najib
% Such a traversal identifies only which cluster it would be. That traversal does not encode (or compress) the cluster-center. For discussing compression and Shannon entropy, we want to encode/compress cluster-centers using the hierarchical relations from the tree in such a way that the child-center is recoverable from a parent-center and the encoding.
% I see that the term `encode' is overloaded here.

\subsection{Scaling Behavior of Cluster Radii}
\label{subsec:methods:radii-scaling-behavior}

We show that the radii of clusters are guaranteed to decrease after, at most, every $d$ recursive applications of Partition.
The Partition algorithm is described in Section~\ref{subsec:methods:partition}.

The Manifold Hypothesis indicates that the instances in the dataset follow a $d$-dimensional distribution with $d \ll D$ where $D$ is the dimensionality of the embedding space.
We can measure the dimensionality of this low-dimensional distribution using the local fractal dimension of the dataset.
Thus, we can describe this distribution by choosing some set of $d$ mutually orthogonal axes.

Let $2r$ be the maximum pairwise distance among the instances in the dataset.
We choose the axes such that the two points that are $2r$ apart lie along one of the axes.
Thus, a $d$-dimensional hyper-sphere of radius $r$ would bound the dataset.
In the worst case, e.g. with a distribution that fills the $d$-sphere, our axes will be such that $2r$ is the maximum pairwise distance \textit{along every axis}.
Such a distribution would also produce a balanced clustering.

Partition will select a maximally distant pair of points to use as poles, i.e. it will choose one of the $d$ axes along which to split the cluster into two children.
After one Partition, the maximum pairwise distance along that axis will be bounded above by $r$.
The next recursive Partition will select another of the $d$ axes.
Thus, after at most $d$ Partitions, the maximum pairwise distance along each axis will be bounded above by $r$.
The overall maximum pairwise distance, i.e. \textit{not restricted} to be along one axis, will be bounded above by $r \sqrt{2}$ by, for example, two instances that lie at the extrema of different axes.

Thus, starting with a Cluster $C$ of radius $r$, after at most $d$ Partitions, the descendants of $C$ will each have radius bounded above by $\frac{r}{\sqrt{2}}$.
In other words, cluster radii are guaranteed to decrease by a multiplicative factor of $\frac{1}{\sqrt{2}}$ after at most $d$ Partitions where $d$ is the local fractal dimension of the manifold occupied by the data.

Note that, in practice, we never see a balanced clustering.
Instead, Partition produces unbalanced trees due to the varying density of sampling in different regions of the manifold and the low-dimensional ``shape'' of the manifold.
Further, the cluster radii decrease by a factor much larger than $\frac{1}{\sqrt{2}}$ and they do so every one or two partitions rather than after $d$ partitions.
See Section~\ref{sec:results} for observations of radii-scaling on several datasets.


\subsection{Back to Shannon Entropy}
\label{subsec:methods:back-to-shannon-entropy}

Starting with the Hierarchical Shannon Entropy defined in Equation~\ref{eq:hierarchical-shannon-entropy}, we can now further analyze the case of encoding a cluster using its children.

In the worst case, we would have a balanced clustering.
Starting with cluster $C$ and applying $d$ recursive partitions would give us $2^d$ descendants with $|C| \cdot 2^{-d}$ instances each.
If cluster $C$ has radius $r$ then each of the $2^d$ descendants will have radius $\frac{r}{\sqrt{2}}$.

In the general case, let $T(C, d)$ be the subtree rooted at $C$ with depth $d$ and up-to $2^d$ leaves.
Since we cannot partition a cluster containing only one instance, we can bound the number of leaves, i.e. $2^d \leq |C|$.
Let the leaves be indexed with integers $j \in [1, 2^{d + 1} - 1]$ as obtained by a breadth-first traversal of the tree, i.e. the root has index $1$, a cluster with index $i < 2^d$ has children with indices $2i$ and $2i + 1$, and a cluster with index $i \geq 2^d$ is a leaf-cluster with no children.
Let the center of each child-cluster be encoded in terms of the center of its parent-cluster.
Therefore, $H(T)$, the Shannon Entropy of $T(C, d)$, is given by
\begin{align*}
    H(T) &= H(c_2 - c_1) + H(c_3 - c_1) + H(c_4 - c_2) + H(c_5 - c_2) \\
    & \ \ \ \ + H(c_6 - c_3) + H(c_7 - c_3) + \dots + H(c_{2^{d + 1} - 1} - c_{2^d})
\end{align*}
i.e. the sum of all encodings of child-centers in terms of their parent-centers, where $c_1$ is the center of the root cluster.

Let us re-index the \textit{leaves} in $T(C, d)$ with integers $j \in [1, 2^d]$.
The second case in Equation~\ref{eq:hierarchical-shannon-entropy} becomes:
\begin{align*}
    & H(T) + \sum H(C_j) \\
    = \ & H(T) + \sum |C_j| \cdot H(y_j - c_j)
\end{align*}

From the analysis in Section~\ref{subsec:methods:radii-scaling-behavior}, we have that $H(y_j - c_j) \leq \frac{1}{\sqrt{2}} H(y - c) \ \forall \ j \in [1, 2^d]$.
Thus, we continue:
\begin{align*}
    \leq \ & H(T) + \frac{1}{\sqrt{2}} \cdot H(y - c) \cdot \sum |C_j| \\
    = \ & H(T) + \frac{1}{\sqrt{2}} \cdot H(y - c) \cdot |C|
\end{align*}

Finally, we rewrite Equation~\ref{eq:hierarchical-shannon-entropy} as
\begin{align}
    \label{eq:hierarchical-shannon-entropy-2}
    H(C) &\leq H(c - c_{parent}) + \min \begin{cases}
        |C| \cdot H(y - c) \\
        |C| \cdot H(y - c) \cdot \frac{1}{\sqrt{2}} + H(T)
    \end{cases}
\end{align}

To encode an entire dataset, we start with the dataset contained in a single root cluster.
We then check between the two cases for Equation~\ref{eq:hierarchical-shannon-entropy-2}.
If the second case provides the smaller Shannon Entropy, then we recursively apply Equation~\ref{eq:hierarchical-shannon-entropy-2} to the leaves of $T(C,d)$.

Thus, we have a measurable trade-off when encoding the instances in a cluster.
This trade-off is calculated using the subtree rooted at that cluster up to a low depth equal to the local fractal fractal dimension of that cluster.

When starting out encoding the dataset in this fashion, for low depths in the tree, we expect that using subtrees to encode the instances will lead to great savings in the number of bits used.
As we continue deeper into the tree, we would start seeing diminishing returns for using subtrees.
Eventually, either 
the difference will be so small so as not to be worth the computational cost of Partition, 
or the two cases will equalize, 
or the first case will become the better choice.
At such a depth in the tree, we can encode the instances in a cluster using the cluster-center rather than the subtree.
The center of that cluster can be encoded in terms of the center of its parent and so on back up the tree to the root.

Notably, such a depth need not be uniform across the tree.
Indeed, the depth can adapt to any variations in sampling density and local fractal dimension in different regions of the manifold.
