\section{Introduction}
\label{sec:introduction}

Researchers are collecting data at an unprecedented rate.
In many fields, datasets are growing exponentially, and this increase in the rate of data collection outpaces improvements in computing performance as predicted by Moore's Law~\cite{kahn2011future}.
This indicates that the performance of computing systems will not keep pace with the growth of data.
Often dubbed ``the Big Data explosion,'' this phenomenon has created a need for better algorithms to analyze large datasets.

Examples of large datasets include genomic databases, time-series data such as radio frequency signals, and neural network embeddings.
Large language models such as GPT~\cite{2020arXiv200514165B, OpenAI2023GPT4TR} and LLAMA-2~\cite{Touvron2023Llama2O}, and image embedding models~\cite{radford2021learning, dosovitskiy2020image} are a common source of neural network embeddings.
Among biological datasets,  SILVA 18S~\cite{10.1093/nar/gks1219} contains ribosomal DNA sequences of approximately 2.25 million genomes; the longest of these individual sequences is 3,712 amino acids, but in a multiple sequence alignment, the length becomes 50,000 letters.
Among time-series datasets, the RadioML dataset~\cite{oshea2018radioml} contains approximately 2.55 million samples of synthetically-generated signals of different modulation modes.

Many researchers are interested in similarity search on these datasets.
Similarity search enables a variety of applications, including recommendation~\cite{annoy} and classification systems~\cite{suyanto2022knnclassifier}.
As the cardinalities and dimensionalities of datasets have grown, however, efficient and accurate similarity search has become challenging;
even state-of-the-art algorithms exhibit a steep tradeoff between recall and throughput~\cite{malkov2016hnsw, johnson2019billion, annoy, aumuller2020ann}.

Given some measure of similarity between data points, there are two common definitions of similarity search: $k$-nearest neighbor search ($k$-NN) and $\rho$-nearest neighbor search ($\rho$-NN).
$k$-NN search aims to find the $k$ most similar points to a query, while $\rho$-NN search aims to find all points within a similarity threshold $\rho$ of a query.
Previous works have used the term \textit{approximate} search to refer to $\rho$-NN search, but in this paper, we reserve the term \textit{approximate} for algorithms that do not exhibit perfect recall.
In contrast, an \textit{exact} search algorithm exhibits perfect recall.

$k$-NN search is one of the most ubiquitous classification and recommendation methods in use~\cite{fix1952discriminatory, cover1967nearest}.
Na\"{i}ve implementations of $k$-NN search, whose time complexity is linear in the dataset's cardinality, prove prohibitively slow for large datasets.
While fast algorithms for $k$-NN search on large datasets exist, they are often approximate~\cite{gao2023high}, and while approximate search may be sufficient for some applications, the need for efficient and \textit{exact} search remains~\cite{ukey2023survey}.
For example, for a majority voting classifier, approximate $k$-NN search may agree with exact $k$-NN search for large values of $k$, but may be sensitive to local perturbations for smaller values of $k$.
This is especially true when classes are not well-separated~\cite{zhang2022imbalanced}.
Further, there is evidence that distance functions that do not obey the triangle inequality, such as cosine distance, perform poorly for $k$-NN search in biomedical settings~\cite{hu2016distance};
this suggests that approximate $k$-NN search could perform poorly in such contexts.

This paper introduces CAKES (CLAM-Accelerated $K$-NN Entropy-Scaling Search), a set of three novel algorithms for exact $k$-NN search.
We also compare CAKES against several current algorithms; namely, FAISS~\cite{johnson2019billion}, HNSW~\cite{malkov2016hnsw}, and ANNOY~\cite{annoy}, on datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
We further benchmark CAKES on a large genomic dataset, the SILVA 18S dataset~\cite{10.1093/nar/gks1219}, using Levenshtein~\cite{levenshtein1966binary} distance on unaligned genomic sequences, and a radio frequency dataset, RadioML~\cite{oshea2018radioml}, using Dynamic Time Warping (DTW)~\cite{gold2018dynamic} distance on complex-valued time-series.
Finally, we compare real-world datasets to an artificially-generated dataset obeying simple statistical properties.


\subsection{Related Works}
\label{sec:intoduction:related-works}

Recent $k$-nearest neighbor search algorithms designed to scale with the exponential growth of data include Hierarchical Navigable Small World networks (HNSW)~\cite{malkov2016hnsw}, InVerted File indexing (FAISS-IVF)~\cite{faissivf}, random projection and tree building (ANNOY)~\cite{annoy}, and entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}. However, some of these algorithms do not provide exact search (as defined in Section \ref{sec:introduction} above).

Hierarchical Navigable Small World networks~\cite{malkov2016hnsw} rely on navigable small world (NSW) networks~\cite{kleinberg2000navigation, boguna2009navigability} and skip lists.
HNSW builds a multi-layered graph of the dataset.
A query point and each data point are inserted into the graph and joined by an edge to the $M$ nearest nodes in the in the graph, where $M$ is a tunable parameter.
The highest layer in which an element can be placed is determined randomly with an exponentially-decaying probability distribution.
Search starts at the highest layer and descends to the lowest layer, greedily following a path of edges to the nearest node, until reaching the query point.
To improve accuracy, the $efSearch$ hyperparameter can be changed to specify the number of closest nearest neighbors to the query vector to be found at each layer.

InVerted File indexing (IVF)~\cite{faissivf, sacks1987multikey, kent1990signature} clusters data into high-dimensional Voronoi cells, and whichever cell a query point falls into is then searched exhaustively, similarly to an early precursor to our work~\cite{yu2015entropy}.
The number of cells used is governed by the $n_{list}$ parameter.
Increasing this parameter decreases the number of points being exhaustively searched, so it improves speed at the cost of accuracy.
To mitigate accuracy issues caused by a query point falling near a cell boundary, the algorithm has a tunable parameter $n_{probe}$ that specifies the number of additional adjacent or nearby cells to search.

ANNOY~\cite{annoy} relies on random projection and tree building for approximate $k$-NN search.
At each intermediate node of the tree, two points are randomly sampled from the space, and the hyperplane equidistant from them is chosen to divide the space into two subspaces.
This process iterates to create a forest of trees, and the number of iterations is a tunable parameter.
At search time, one can increase the number of trees to be searched to improve recall at the cost of speed.


\subsection{Entropy-Scaling Search}
\label{sec:intoduction:entropy-scaling-search}

The entropy-scaling search paradigm exploits the geometric and topological structure inherent in large datasets.
Importantly, as suggested by their name, entropy-scaling search algorithms exhibit asymptotic complexity that scales with topological properties of the dataset, instead of its cardinality, when the dataset has a manifold structure.
In 2019, we introduced CHESS (Clustered Hierarchical Entropy-Scaling Search)~\cite{ishaq2019clustered}, which extended entropy-scaling $\rho$-NN search from a flat clustering approach to a hierarchical clustering approach.
CLAM (Clustering, Learning and Approximation with Manifolds), originally developed to allow ``manifold mapping'' for anomaly detection~\cite{ishaq2021clustered}, is a refinement of the clustering algorithm from CHESS.
In this paper, we introduce CAKES, a set of three entropy-scaling algorithms for $k$-NN search, implemented in the Rust programming language.

We also provide a theoretical analysis of the time complexity of CAKES's algorithms in Sections~\ref{sec:methods:knn-search:repeated-rnn-complexity} and~\ref{sec:methods:knn-search:complexity-of-sieve-methods}.
These analyses are not worst-case analyses in the traditional sense, as they do not assume the worst possible dataset, namely, a uniform distribution.
Given that CAKES's algorithms are intended to be used on datasets with a manifold structure, complexity analysis assuming a uniform distribution of data would be uninformative.
As a result, we assume in our analyses that the dataset has a manifold structure, as quantified by low fractal dimension and metric entropy.
For our purposes, it makes more sense to analyze the expected performance of CAKES's algorithms under these assumptions.
