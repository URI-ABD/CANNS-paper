\section{Introduction}
\label{sec:introduction}

Researchers are collecting data at an unprecedented scale.
In many fields, datasets grow exponentially, and this increase in the rate of data collection outpaces improvements in computing performance as predicted by Moore's Law~\cite{kahn2011future}.
This indicates that the performance of computing systems will not keep pace with the growth of data.
Often dubbed ``the Big Data explosion,'' this phenomenon has created a need for better algorithms to analyze large datasets.

Examples of large datasets include genomic databases, time-series data such as radio frequency signals, and neural network embeddings.
Large language models such as GPT~\cite{2020arXiv200514165B, OpenAI2023GPT4TR} and LLAMA-2~\cite{Touvron2023Llama2O}, and image embedding models~\cite{radford2021learning, dosovitskiy2020image} are a common source of neural network embeddings.
Among biological datasets,  SILVA 18S~\cite{10.1093/nar/gks1219} contains ribosomal DNA sequences of approximately 2.25 million genomes with an aligned length of 50,000 letters.
Among time-series datasets, the RadioML dataset~\cite{oshea2018radioml} contains approximately 2.55 million samples of synthetically-generated signals of different modulation modes.

Many researchers are interested in similarity search on these datasets.
Similarity search enables a variety of applications, including recommendation~\cite{annoy} and classification systems~\cite{suyanto2022knnclassifier}.
As the cardinalities and dimensionalities of datasets have grown, however, efficient and accurate similarity search has become challenging;
even state-of-the-art algorithms exhibit a steep tradeoff between recall and throughput~\cite{Malkov2016EfficientAR, johnson2019billion, annoy, aumuller2020ann}.

Given some measure of similarity between data points, there are two common definitions of similarity search: $k$-nearest neighbor search ($k$-NN) and $\rho$-nearest neighbor search ($\rho$-NN).
$k$-NN search aims to find the $k$ most similar points to a query, while $\rho$-NN search aims to find all points within a similarity threshold $\rho$ of a query.
Previous works have used the term \textit{approximate} search to refer to $\rho$-NN search, but in this paper, we reserve the term \textit{approximate} for algorithms which do not exhibit perfect recall.
In contrast, an \textit{exact} search algorithm exhibits perfect recall.

$k$-NN search is one of the most ubiquitous classification and recommendation methods in use~\cite{fix1952discriminatory, cover1967nearest}.
Na\"{i}ve implementations of $k$-NN search, whose time complexity is linear in the dataset's cardinality, prove prohibitively slow for large datasets.
While fast algorithms for $k$-NN search on large datasets exist, they are often approximate~\cite{gao2023high}, and while approximate search may be sufficient for some applications, the need for efficient and \textit{exact} search remains~\cite{ukey2023survey}.
For example, for a majority voting classifier, approximate $k$-NN search may agree with exact $k$-NN search for large values of $k$, but may be sensitive to local perturbations for smaller values of $k$.
This is especially true when classes are not well-separated~\cite{zhang2022imbalanced}.
Further, there is evidence that distance functions which do not obey the triangle inequality, such as cosine distance, perform poorly for $k$-NN search in biomedical settings~\cite{hu2016distance};
this suggests that approximate $k$-NN search could perform poorly in such contexts.

$\rho$-NN search also has a variety of applications.
For example, one could search for all genomes within a maximum edit distance of a query genome to find evolutionarily-related organisms~\cite{budowski2010fragbag}.
One could also search for all words within a maximum edit distance of a misspelled word to suggest corrections~\cite{ukkonen1985algorithms}.
Given an advertisement and a database of user profiles, one could search for all users whose profiles are ``similar enough'' to target the advertisement~\cite{zhang2020privacy}.
GPS and other location-based services use $\rho$-NN search to find nearby points of interest~\cite{zhang2020privacy} to a user's location.
%In all of these cases, approximate $\rho$-NN search may be sufficient, but exact $\rho$-NN search is preferable.

This paper introduces CAKES (CLAM-Accelerated $K$-NN Entropy Scaling Search), a set of three novel algorithms for exact $k$-NN search.
We also present improvements to the clustering and $\rho$-NN search algorithms in CHESS~\cite{ishaq2019clustered}, as well as improved genericity across distance functions.
We provide a comparison to several current algorithms for similarity search; namely, FAISS~\cite{johnson2019billion}, HNSW~\cite{malkov2016hnsw}, and ANNOY~\cite{annoy}, on datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
We further benchmark CAKES on a large genomic dataset, the SILVA 18S dataset~\cite{10.1093/nar/gks1219}, using Levenshtein~\cite{levenshtein1966binary} distance on unaligned genomic sequences, and a radio frequency dataset, RadioML~\cite{oshea2018radioml}, using Dynamic Time Warping (DTW)~\cite{gold2018dynamic} distance on complex-valued time-series.
Finally, we compare real-world datasets to an artificially-generated dataset obeying simple statistical properties.


\subsection{Related Works}
\label{sec:intoduction:related-works}

Recent $k$-nearest neighbor search algorithms designed to scale with the exponential growth of data include Hierarchical Navigable Small World networks (HNSW)~\cite{Malkov2016EfficientAR}, InVerted File indexing (FAISS-IVF)~\cite{faissivf}, random projection and tree building (ANNOY)~\cite{annoy}, and entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}. However, some of these algorithms do not provide exact search (as defined in Section \ref{sec:introduction} above).

Hierarchical Navigable Small World networks~\cite{Malkov2016EfficientAR} relies on navigable small world (NSW) networks~\cite{kleinberg2000navigation, boguna2009navigability} and skip lists.
HNSW builds a multi-layered graph of the dataset.
A query point and each data point are inserted into the graph and joined by an edge to the $M$ nearest nodes in the in the graph, where $M$ is a tunable parameter.
The highest layer in which an element can be placed is determined randomly with an exponentially-decaying probability distribution.
Search starts at the highest layer and descends to the lowest layer, greedily following a path of edges to the nearest node, until reaching the query point.
To improve accuracy, the $efSearch$ hyperparameter can be changed to specify the number of closest nearest neighbors to the query vector to be found at each layer.

InVerted File indexing (IVF)~\cite{faissivf, sacks1987multikey, kent1990signature} clusters data into high-dimensional Voronoi cells, and whichever cell a query point falls into is then searched exhaustively, similarly to an early precursor to our work~\cite{yu2015entropy}.
The number of cells used is governed by the $n_{list}$ parameter.
Increasing this parameter decreases the number of points being exhaustively searched, so it improves speed at the cost of accuracy.
To mitigate accuracy issues caused by a query point falling near a cell boundary, the algorithm has a tunable parameter $n_{probe}$, which specifies the number of additional adjacent or nearby cells to search.

ANNOY~\cite{annoy} relies on random projection and tree building for approximate $k$-NN search.
At each intermediate node of the tree, two points are randomly sampled from the space, and the hyperplane equidistant from them is chosen to divide the space into two subspaces.
This process iterates to create a forest of trees, and the number of iterations is a tunable parameter.
At search time, one can increase the number of trees to be searched to improve recall at the cost of speed.


\subsection{Entropy Scaling Search}
\label{sec:intoduction:entropy-scaling-search}

The entropy-scaling search paradigm exploits the geometric and topological structure inherent in large datasets.
Importantly, as suggested by their name, entropy-scaling search algorithms exhibit asymptotic complexity that scales with topological properties of the dataset, instead of its cardinality.
In 2019, we introduced CHESS (Clustered Hierarchical Entropy-Scaling Search)~\cite{ishaq2019clustered}, which extended entropy-scaling $\rho$-NN search from a flat clustering approach to a hierarchical clustering approach.
CLAM (Clustering, Learning and Approximation with Manifolds), originally developed to allow ``manifold mapping'' for anomaly detection~\cite{ishaq2021clustered}, is a refinement of the clustering algorithm from CHESS.
In this paper, we introduce CAKES, a set of three entropy-scaling algorithms for $k$-NN search, implemented in the Rust programming language.
