\section{Introduction}
\label{sec:introduction}
Often dubbed "the Big Data explosion", rapidly advancing technologies have enabled researchers to collect complex data on an unprecedented scale. 
The Sloan Digital Sky Survey (SDSS), for example, has collected near-infrared spectra of approximately 430,000 stars in approximately 9,000 dimensions~\cite{alam2015eleventh}. 
Add sentence about MaNGA here: 10,000 galaxies with 2,700 deg$^2$, resolution $R=2,000$ for $\lambda \in [360, 1000) nm$.

Biology has also seen an influx of large, high-dimensional datasets. Silva 18S contains ribosomal DNA sequences of approximately 2.25 
million genomes with aligned length of 50,000 letters. (Do we need to look at NR99??) 

In many cases, the increase in data outpaces the rate of improvement in computing capacity predicted by Moore's Law~\cite{moore1965cramming}, indicating 
that computer hardware will not "catch up" to our computational needs in the near future. As a result, analyzing and learning from these large datasets 
requires better algorithms. 

The ability to perform similarity search on these datasets enables a variety of applications, including recommendation and classification systems. 
Additionally, with the rise of large language models such as GPT-3~\cite{brown2020language}, similarity search can become a useful tool for 
understanding and verifying the validity of model outputs. 
As the sizes and dimensionalities of datasets have grown, efficient and accurate similarity search has become excessively challenging; 
state-of-the-art algorithms exhibit a steep recall versus speed tradeoff~\cite{ishaq2019clustered}.
In particular, k-nearest neighbors (kNN) search is one of the most pervasive supervised classification methods in use~\cite{fix1952discriminatory, cover1967nearest}.

Naive implementations of kNN, whose asymptotic complexity is a function of the dataset's cardinality, 
prove prohibitively slow for large datasets. While algorithms for kNN on large datasets exist, most of them are approximate and fail to 
exploit the inherent structure and redundancy often present in big datasets. Approximate search is sufficient for many applications, but the 
need for efficient, exact search remains.

Recent approaches to tackling the exponential growth of data include locality-sensitive hashing~\cite{bingham2001random}, 
clever indexing techniques such as the FM Index~\cite{ferragina2005indexing}, and entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}. 
Entropy-scaling search is a paradigm for similarity search that exploits the inherent structure and redundancy often present in big datasets.
Importantly, as suggested by their name, entropy-scaling search algorithms have asymptotic complexity that scales with geometric properties of the dataset,
rather than its cardinality. In 2019, we introduced CHESS (CLustered Hierarchical Entropy-Scaling Search), which extended entropy-scaling $\rho$-nearest 
neighbors search to a hierarchical clustering approach. In this paper, we introduce CLAM (Clustered Learning of Approximate Manifolds), and CAKES (CLAM-Accelerated K-nearest-neighbor 
Entropy-scaling Search). CLAM, like CHESS, is a hierarchical clustering algorithm (figure out and explain improvements upon CHESS present in CLAM if there are any algorithmic ones)
that can facilitate a variety of algorithms for big data. Using the tree constructed by CLAM, CAKES builds upon the capabilities of CHESS to perform 
k-nearest neighbors search in addition to $\rho$-nearest neighbors search.


At some point, need to address the fact that exact search is important for small k and approximate 
is maybe more ideal for large k. Try to find citation for this. 

This paper focuses on:
\begin{itemize}
    \item Improvements and extensions of CHESS.
    \item Accelerated $\rho$-nearest and k-nearest neighbors search.
    \item Proof for how cluster-radii fall as we descend along the tree.
    \item A variety of distance functions including euclidean, manhattan, cosine, jaccard, hamming, levenshtein, needleman-wunch, and any user-defined function.
\end{itemize}

Comparisons for:
\begin{itemize}
    \item Search time vs FAISS and FALCONN and other in ann-benchmarks suite.
\end{itemize}
