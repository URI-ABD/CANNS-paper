\section{Introduction}
\label{sec:introduction}

Researchers are collecting data at an unprecedented scale.
In many fields, the sizes of datasets are growing exponentially, and this increase in the rate of data collection outpaces the rate of improvements in computing performance as predicted by Moore's Law~\cite{brescia2012extracting}.
This indicates that computer architecture will not ``catch up'' to computational needs in the near future.
Often dubbed ``the Big Data explosion,'' this phenomenon has led to a need for better algorithms for analyzing such datasets.

Examples of large datasets include neural network embeddings, genomic databases and time-series data such as radio frequency signals.
Large language models such as GPT~\cite{2020arXiv200514165B, OpenAI2023GPT4TR} and LLAMA-2~\cite{Touvron2023Llama2O}, and image analysis models such as {\color{red} [NAME]~[CITE] and [NAME]~[CITE]} are a common source of neural network embeddings.
These embeddings are often high-dimensional, and the sizes of training and inference datasets for such networks are growing exponentially.
Among Biological datasets, the GreenGenes project~\cite{desantis2006greengenes} provides a multiple-sequence alignment of over one million bacterial 16S sequences, each 7,682 characters in length while Silva 18S~\cite{10.1093/nar/gks1219} contains ribosomal DNA sequences of approximately 2.25 million genomes with an aligned length of 50,000 letters.
Among time-series datasets, the RadioML dataset~\cite{oshea2018radioml} contains approximately 2.55 million samples of synthetically generated signal captures of different modulation modes over a range of SNR levels.

Many researchers are especially interested in similarity search on these datasets. 
Similarity search enables a variety of applications, including recommendation~\cite{annoy} and classification systems~\cite{suyanto2022knnclassifier}. 
As the cardinalities and dimensionalities of datasets have grown, however, efficient and accurate similarity search has become extremely challenging; 
even state-of-the-art algorithms exhibit a steep tradeoff between recall and throughput~\cite{Malkov2016EfficientAR, johnson2019billion, annoy, aumuller2020ann}.

Given some measure of similarity between data points, e.g. a distance function, there are two common definitions similarity search: $k$-nearest neighbor search ($k$-NN) and $\rho$-nearest neighbor search ($\rho$-NN).
$k$-NN search aims to find the $k$ most similar points to a query point, while $\rho$-NN search aims to find all points within a similarity threshold $\rho$ of a query point.

Previous works have used the term \emph{approximate} search to refer to $\rho$-NN search, but in this paper, we reserve the term \emph{approximate} for search algorithms which do not exhibit perfect recall when compared to a na\"{i}ve linear search.
In contrast, an \emph{exact} search algorithm exhibits perfect recall.

$k$-NN search is one of the most ubiquitous classification and recommendation methods in use~\cite{fix1952discriminatory, cover1967nearest}.
Na\"{i}ve implementations of $k$-NN search, whose time complexity is linear in the dataset's cardinality, prove prohibitively slow for large datasets because their cardinalities are growing exponentially.
While fast algorithms for $k$-NN search on large datasets do exist, most do not exploit the geometric and topological structure inherent in these datasets.
Further, such algorithms are often approximate and while approximate search may be sufficient for some applications, the need for efficient and \emph{exact} search remains.

For example, for a majority voting classifier, approximate $k$-NN search may agree with exact $k$-NN search for large values of $k$, but may be sensitive to local perturbations for smaller values of $k$.
This is especially true when classes are not well-separated~\cite{zhang2022imbalanced}.
Further, there is some evidence that distance functions which do not obey the triangle inequality, such as cosine distance, perform poorly for $k$-NN search in biomedical settings~\cite{hu2016distance};
this suggests that approximate $k$-NN search could exhibit suboptimal classification accuracy in such contexts.

{\color{red} TODO: Talk about common uses of $\rho$-NN search, as we do with $k$-NN search above.}

This paper focuses on three novel algorithms for exact $k$-NN search.
We also present some improvements to the clustering and $\rho$-NN search algorithms in CHESS, as well as improved genericity across distance functions.
We provide a comparison of CAKES to several state-of-the-art algorithms in similarity search, FAISS~\cite{johnson2019billion}, HNSW~\cite{malkov2016hnsw}, and ANNOY~\cite{annoy}, on several datasets from the ANN-benchmarks suite~\cite{aumuller2020ann}.
We also benchmark CAKES on a genomic dataset, the Silva 18S dataset~\cite{10.1093/nar/gks1219} using Levenshtein~\cite{levenshtein1966binary} distance on unaligned genomic sequences, and a radio frequency dataset, the RadioML dataset~\cite{oshea2018radioml} using Dynamic Time Warping (DTW)~\cite{gold2018dynamic} distance on complex-valued time-series.


\subsection{Related Works}
\label{sec:intoduction:related-works}

Recent search algorithms for tackling the exponential growth of data include hierarchical navigable small world networks (HNSW)~\cite{Malkov2016EfficientAR}, InVerted File indexing (FAISS-IVF)~\cite{faissivf}, random projection and tree building (ANNOY)~\cite{annoy}, and entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}.


\subsubsection{HNSW}
\label{sec:introduction:related-works:hnsw}

HNSW is an approximate $k$-NN search method based on navigable small world (NSW) graphs and skip lists. 
Similar to the NSW algorithm, HNSW builds a graph of the dataset, but unlike NSW, the graph is multi-layered.
The query point and each data point are inserted into the graph one at a time, and, upon insertion, the element is joined by an edge to the $M$ nearest nodes in the in the graph, where $M$ is a tunable parameter. 
The highest layer in which an element can be placed is determined randomly with an exponentially decaying probability distribution.
Search starts at the highest layer and descends to the lowest layer, greedily following a path of edges to the nearest node, until reaching the query point. 
To improve accuracy, the efSearch hyperparameter can be changed to specify the number of closest nearest neighbors to the query vector to be found at each layer. 


\subsubsection{FAISS-IVF}
\label{sec:introduction:related-works:faiss-ivf}

InVerted File indexing (IVF) is a method for approximate $k$-NN search. 
The dataset is clustered into a high dimensional Voronoi diagram, and whichever cell the query point falls into is then searched exhaustively.
The number of cells created is governed by the $n_{list}$ parameter. 
Increasing this parameter decreases the number of points being exhaustively searched, so it improves speed at the cost of accuracy.
To mitigate accuracy issues caused by a query point falling near a cell boundary, IVF has a tunable parameter $n_{probe}$, which specifies the number of cells to search.


\subsubsection{ANNOY}
\label{sec:introduction:related-works:annoy}

Annoy is an approximate $k$-NN search method based on random projection and tree building.
At each intermediate node of the tree, two points are sampled from the space, and the hyperplane equidistant from them is chosen to divide the space into two subspaces.
This process is repeated multiple times to create a forest of trees, and the number of times the process is repeated is a tunable parameter.
{\color{red} TODO: I'm still unclear what they are doing with these trees.}

\subsubsection{Entropy-Scaling Search}
\label{sec:introduction:related-works:entropy-scaling-search}

This search paradigm exploits the geometric and topological structure inherent in large datasets.
Importantly, as suggested by their name, entropy-scaling search algorithms have asymptotic complexity that scales with topological properties (such as the \emph{metric entropy} and \emph{local fractal dimension}, as defined in Section~\ref{sec:methods}) of the dataset, instead of its cardinality.
In 2019, we introduced CHESS (Clustered Hierarchical Entropy-Scaling Search), which extended entropy-scaling $\rho$-NN search from a flat clustering approach to a tree-based hierarchical clustering approach.
In this paper, we introduce CAKES (CLAM-Accelerated $K$-nn Entropy-scaling Search).
CLAM (Clustering, Learning and Approximation with Manifolds), originally developed to allow ``manifold mapping'' for anomaly detection~\cite{ishaq2021clustered}, is a refinement of the clustering algorithm from CHESS.
Using the cluster tree constructed by CLAM, CAKES extends CHESS to perform $k$-NN search and improves the performance of CHESS's $\rho$-NN search.
