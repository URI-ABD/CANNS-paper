\section{Introduction}
\label{sec:introduction}
Often dubbed "the Big Data explosion," rapidly advancing technologies have enabled researchers to collect complex data on an unprecedented scale. 
This is especially true in the field of astronomy. 
The Sloan Digital Sky Survey (SDSS), for example, in its APOGEE dataset, has collected near-infrared spectra of approximately 430,000 stars in approximately 9,000 dimensions~\cite{alam2015eleventh}. 
Their MaNGA dataset also contains spectral measurements across the face of each of about 10,000 galaxies across 2,700 deg$^2$.

Biology has also seen an influx of large, high-dimensional datasets. Silva 18S contains ribosomal DNA sequences of approximately 2.25 
million genomes with aligned length of 50,000 letters. 

% (TODO: Do we need to talk about NR99; it seems like more of the same but 
% with removed redundancy; not sure if that is helpful to us...) 
% No we don't... I'll explain why if you want.

In many fields, data collection rates outpace the rate of computing performance improvements predicted by Moore's Law~\cite{moore1965cramming}, indicating 
that computer architecture will not "catch up" to our computational needs in the near future. As a result, analyzing and learning from these large datasets 
requires better algorithms. 

Many researchers would like the ability to perform similarity search on these datasets. 
Similatity search enables a variety of applications, including recommendation and classification systems. 
Additionally, with the rise of large language models such as GPT-3~\cite{brown2020language}, similarity search can become a useful tool for 
understanding and verifying the validity of model outputs. 
As the sizes and dimensionalities of datasets have grown, however, efficient and accurate similarity search has become extremely challenging; 
even state-of-the-art algorithms exhibit a steep recall versus speed tradeoff~\cite{ishaq2019clustered}.


In particular, $k$-nearest neighbors (kNN) search is one of the most pervasive supervised classification methods in use~\cite{fix1952discriminatory, cover1967nearest}; most typically classification relies upon a majority vote among the $k$ nearest neighbors to a query.
Naive implementations of kNN, whose time complexity is linear in the dataset's cardinality, 
prove prohibitively slow for large datasets. While algorithms for fast kNN on large datasets exist, most of them are approximate and do not 
exploit the inherent structure and redundancy often present in big datasets. Approximate search is sufficient for many applications, but the 
need for efficient, \emph{exact} search remains.
In particular, while approximate kNN may agree with exact kNN in a majority vote for large values of $k$, it may be sensitive to local perturbations with smaller values of $k$.

Recent approaches to tackling the exponential growth of data include locality-sensitive hashing~\cite{bingham2001random}, 
clever indexing techniques such as the FM Index~\cite{ferragina2005indexing}, and entropy-scaling search~\cite{yu2015entropy, ishaq2019clustered}. 
Entropy-scaling search is a paradigm for similarity search that exploits the inherent structure and redundancy often present in big datasets.
Importantly, as suggested by their name, entropy-scaling search algorithms have asymptotic complexity that scales with geometric properties (such as the 
\emph{metric entropy} and \emph{local fractal dimension}, defined in Section 2) of a dataset,
rather than its cardinality. In 2019, we introduced CHESS (Clustered Hierarchical Entropy-Scaling Search), which extended entropy-scaling $\rho$-nearest 
neighbors search to a hierarchical clustering approach. In this paper, we introduce CAKES (CLAM-Accelerated K-nearest-neighbor 
Entropy-scaling Search). CLAM (Clustered Learning of Approximate Manifolds), developed to allow ``manifold mapping'' for anomaly detection~\cite{ishaq2021clustered} is a refinement of the CHESS clustering algorithm. Using the cluster tree constructed by CLAM, CAKES builds upon the capabilities of CHESS to perform 
$k$-nearest neighbors search in addition to $\rho$-nearest neighbors search.


This paper focuses on improvements to CHESS, accelerated $\rho$- and $k$-nearest neighbors search, proof of a geometric guarantee for clusters, 
and the implementation of a variety of distance functions, including Euclidean, Manhattan, Cosine, Jaccard, Hamming, Levenshtein, Needleman-Wunch, 
and any user-defined function. We also provide a comparison of CAKES to FAISS~\cite{johnson2019billion}, FALCONN~\cite{andoni2015practical}, and
other algorithms in the ann-benchmarks suite~\cite{wang2017ann}.
