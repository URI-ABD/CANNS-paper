\section{Results}
\label{sec:results}

Figures~\ref{fig:results:fashion-mnist-lfd} show the local fractal dimension of.... (TODO fill this in)

%% LFD plots

\begin{figure}
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/fashion-mnist-60000-lfd.png}\\
    \subcaption{Fashion-mnist}
    \label{fig:results:fashion-mnist-lfd}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/glove-25-1183514-lfd.png}\\
    \subcaption{Glove-25}
    \label{fig:results:glove-25-lfd}
    \end{subfigure}
    \vspace{1em}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/sift-1000000-lfd.png}\\
    \subcaption{Sift}
    \label{fig:results:sift-lfd}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/random-1000000-lfd.png}\\
    \subcaption{A random dataset}
    \label{fig:results:random-lfd}
    \end{subfigure}
    \vspace{1em}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/silva-2224640-lfd.png}\\
    \subcaption{Silva 18S metagenomics}
    \label{fig:results:silva-lfd}
    \end{subfigure}%
    \begin{subfigure}[b]{0.47\textwidth}
    \includegraphics[width=0.95\textwidth]{images/lfd_plots/radio-ml-97920-lfd.png}\\
    \subcaption{RadioML}
    \label{fig:results:radioml-lfd}
    \end{subfigure}%

    \caption{Local fractal dimension vs. cluster depth across six datasets, grouped by decile of LFD and weighted by the cardinalities of the clusters.}
    \label{fig:results:lfd-plots}
\end{figure}
%% end LFD plots

Figures~\ref{fig:results:fashion-mnist-scaling}, ~\ref{fig:results:glove-25-scaling}, and ~\ref{fig:results:sift-scaling} show the scaling behavior of CAKES on augmented versions of the following ann-benchmark datasets: fashion-mnist under Euclidean distance, glove-25 under cosine distance, and sift under Euclidean distance. 
Figure~\ref{fig:results:random-scaling} shows the scaling behavior of CAKES on a completely randomly generated dataset with same cardinality and dimensionality as sift (1 million points in 128 dimensions).
The horizontal axis in each figure denotes the multiplier by which we increased the cardinality of the original dataset with synthetic points (see Section \ref{subsec:methods:synthetic-data}). 
The vertical axis denotes throughput in queries per second. Both axes are on a logarithmic scale. In this section, we discuss results only for $k$-NN search with $k = 10$, but similar plots for $k=100$ can be found in our Supplement section. In these plots, we label the recall at each scale for algorithms which do not exhibit perfect recall; if no such labels are given for an algorithm, then that algorithm exhibits perfect recall at all scales.

Tables \ref{table:results:ann-fashion}, \ref{table:results:ann-glove-25}, \ref{table:results:ann-sift}, and \ref{table:results:ann-random} show the throughput and recall of CAKES's algorithms at each cardinality multiplier (i.e., scale) for the fashion-mnist, glove-25, sift, and random datasets respectively. When reporting recall, we use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider
only three decimal places.
Figures~\ref{fig:results:fashion-mnist-scaling}, ~\ref{fig:results:glove-25-scaling}, and ~\ref{fig:results:sift-scaling} show the scaling behavior of CAKES on augmented versions of the following ann-benchmark datasets: fashion-mnist under Euclidean distance, glove-25 under cosine distance, and sift under Euclidean distance. 
Figure~\ref{fig:results:random-scaling} shows the scaling behavior of CAKES on a completely randomly generated dataset with same cardinality and dimensionality as sift (1 million points in 128 dimensions).
The horizontal axis in each figure denotes the multiplier by which we increased the cardinality of the original dataset with synthetic points (see Section \ref{subsec:methods:synthetic-data}). 
The vertical axis denotes throughput in queries per second. Both axes are on a logarithmic scale. In this section, we discuss results only for $k$-NN search with $k = 10$, but similar plots for $k=100$ can be found in our Supplement section. In these plots, we label the recall at each scale for algorithms which do not exhibit perfect recall; if no such labels are given for an algorithm, then that algorithm exhibits perfect recall at all scales.

Tables \ref{table:results:ann-fashion}, \ref{table:results:ann-glove-25}, \ref{table:results:ann-sift}, and \ref{table:results:ann-random} show the throughput and recall of CAKES's algorithms at each cardinality multiplier (i.e., scale) for the fashion-mnist, glove-25, sift, and random datasets respectively. When reporting recall, we use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider
only three decimal places.

In Figure~\ref{fig:results:fashion-mnist-scaling}, which shows results for fashion-mnist, we observe that as the augmentation multiplier increases, the CAKES algorithms (GreedySieve in blue, RepeatedRnn in green, and Sieve in red) exhibit higher throughput than na\"{i}ve linear search, plotted in orange. We also observe that CAKES outperforms state-of-the-art similarity search algorithm FAISS-Flat (plotted in brown) for all cardinality multipliers and outperforms FAISS-IVF (plotted in pink) for all multipliers greater than $10^{5.5}$. Though state-of-the-art algorithms HNSW (plotted in gray), and Annoy (plotted in purple) have higher throughput than CAKES for all multipliers, we note that CAKES exhibits perfect recall for all multipliers, while hnsw and annoy exhibit much lower recall, as shown in Table~\ref{table:results:ann-fashion}. For example, at a cardinality multiplier as low as eight, hnsw and annoy have recall of 0.525 and 0.857 respectively, while CAKES exhibits perfect recall.
Amongst the CAKES algorithms, GreedySieve consistently performs best on this dataset, exhibiting sublinear time performance for nearly all cardinality multipliers. 
In Figure~\ref{fig:results:fashion-mnist-scaling}, which shows results for fashion-mnist, we observe that as the augmentation multiplier increases, the CAKES algorithms (GreedySieve in blue, RepeatedRnn in green, and Sieve in red) exhibit higher throughput than na\"{i}ve linear search, plotted in orange. We also observe that CAKES outperforms state-of-the-art similarity search algorithm FAISS-Flat (plotted in brown) for all cardinality multipliers and outperforms FAISS-IVF (plotted in pink) for all multipliers greater than $10^{5.5}$. Though state-of-the-art algorithms HNSW (plotted in gray), and Annoy (plotted in purple) have higher throughput than CAKES for all multipliers, we note that CAKES exhibits perfect recall for all multipliers, while hnsw and annoy exhibit much lower recall, as shown in Table~\ref{table:results:ann-fashion}. For example, at a cardinality multiplier as low as eight, hnsw and annoy have recall of 0.525 and 0.857 respectively, while CAKES exhibits perfect recall.
Amongst the CAKES algorithms, GreedySieve consistently performs best on this dataset, exhibiting sublinear time performance for nearly all cardinality multipliers. 


With glove-25 (Figure~\ref{fig:results:glove-25-scaling}), we observe that each of CAKES' algorithms outperforms linear search at high cardinality multipliers. Similar to the results for fashion-mnist, we see that CAKES outperforms FAISS-Flat for all cardinality multipliers and outperforms FAISS-IVF for all multipliers greater than $10^{7}$. We also see that while hnsw and annoy have higher throughput than CAKES for all multipliers, CAKES exhibits perfect recall for all multipliers, while hnsw and annoy exhibit much lower recall, as shown in Table~\ref{table:results:ann-glove-25}. For example, at a cardinality multiplier as low as eight, hnsw and annoy have recall of 0.294 and 0.857 respectively, while CAKES exhibits perfect recall.
On this dataset, Repeated $\rho$-NN, plotted in green, is consistently the best-performing algorithm. {\color{red} Revive lfd violin plots to add explanation for this.}



In Figure~\ref{fig:results:sift-scaling}, we observe that while linear search performs best for low cardinality multipliers, our algorithms begin outperforming linear search beginning at a multiplier of approximately 7. 
For this dataset, Sieve, plotted in red, is the highest throughput algorithm for all multipliers greater than 7. 

Figure \ref{fig:results:random-scaling} displays results for a random datset with the same dimensionality and cardinality as Sift (i.e., 128 and 1,000,000 respectively). 
These results illustrate the fact that \emph{ceteris paribus}, absent a manifold structure in the dataset, CAKES's algorithms do not begin to outperform linear search until the cardinality multiplier is around 10. 
Repeated $\rho$-NN shows significantly worse performance on this dataset relative to both the other algorithms and to its own performance on real datasets. 
This is unsurprising, given that Repeated $\rho$-NN relies on a low local fractal dimension around the query in order to quickly home in on the correct radius for $k$ hits. 
With a completely random dataset, whose average local fractal dimension will be near its embedding dimension, Repeated $\rho$-NN will iterate for longer before finding this correct radius. 

Notably, we observe that on all datasets, GreedySieve's throughput stays nearly constant as cardinality increases. This observation warrants further investigation, but it is especially promising given that GreedySieve consistently outperforms linear search for high cardinalities. 
% We also notice an interesting trend with Sieve and Sieve with Separate centers: for all datasets, at both values of $k$, both algorithms exhibit a sharp ``pivot point'' after which throughput starts to increase. 
% While we observe that this pivot point seems to be proportional to the value of $k$, as it often occurs at about a multiplier of 4 when $k=10$ and a mulitplier of 32 when $k=100$, the reason for this behavior requires further investigation.  
Finally, we reiterate that the variation in performance of our algorithms across different datasets and cardinalities support our use of an autotuning function to select the best choice of algorithm. 
% Currently, our autotuning function always uses $k=10$ for its determination, but our observation that the best algorithm often differs between small and large values of $k$ suggests that more sophisticated autotuning is necessary.


\begin{figure}
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/fashion-mnist-knn-10.png}
\subcaption{Fashion-mnist for $k=10$.}
\label{fig:results:fashion-mnist-scaling}
\end{subfigure}%
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/glove-25-knn-10.png}
\subcaption{Glove-25 for $k=10$.  }
\label{fig:results:glove-25-scaling}
\end{subfigure}%
\vspace{1em}
\\
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/sift-knn-10.png}
\subcaption{Sift for $k=10$.    }
\label{fig:results:sift-scaling}
\end{subfigure}%
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/random-knn-10.png}
\subcaption{A random dataset for $k=10$.
}
\label{fig:results:random-scaling}
\end{subfigure}%
\vspace{1em}
\\
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/silva-knn-10.png}
\subcaption{Silva for $k=10$.    }
\label{fig:results:silva-scaling}
\end{subfigure}%
\begin{subfigure}[b]{0.47\textwidth}
\includegraphics[width=0.95\textwidth]{plots/radio-ml-knn-10.png}
\subcaption{RadioML for $k=10$ at SnR = 10dB.
}
\label{fig:results:radioml-scaling}
\end{subfigure}%



\caption{Algorithm performance across six datasets, including a randomly-generated dataset. In each plot, the horizontal axis represents increasing cardinality (size) of the dataset, while the vertical axis represents the throughput in queries per second (higher is better). TODO say something about plots that didn't finish.}
\end{figure}

% 
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=3.4in]{plots/fashion-mnist-knn-10.png}
%     \caption{
%         Algorithm performance by cardinality multiplier on fashion-mnist for $k=10$.
%     }
%     \label{fig:results:fashion-mnist-scaling}
% \end{figure}

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=3.4in]{images/result_plots/deep-image_10_scaling.png}
%     \caption{
%         Deep-image Scaling
%     }
%     \label{fig:results:deep-image-scaling}
% \end{figure}


% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=3.4in]{plots/glove-25-knn-10.png}
%     \caption{
%         Algorithm performance by cardinality multiplier on glove-25 for $k=10$.  }
%     \label{fig:results:glove-25-scaling}
% \end{figure}

% 
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=3.4in]{plots/sift-knn-10.png}
%     \caption{
%         Algorithm performance by cardinality multiplier on sift for $k=10$.    }
%     \label{fig:results:sift-scaling}
% \end{figure}

% We need to redo this plot so that we do not actually augment random datasets but instead take larger and larger 
% random datasets
% 
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=3.4in]{plots/random-knn-10.png}
%     \caption{
%         Algorithm performance by cardinality multiplier on a random dataset for $k=10$.
%     }
%     \label{fig:results:random-scaling}
% \end{figure}


% We did not run any of the competitor's algorithms.
% We lifted the reported numbers from the ann-benchmarks site and their interactive plots.
% We made sure to use the same AWS instance as they did so it is a fair comparison.

% Tables \ref{table:results:ann-10} and \ref{table:results:ann-100} show the results of our benchmarks against FAISS, bruteforce-blas, and HNSW.
% The throughput values for these algorithms comes from the ANN benchmarks website's interactive plots, from which we can assess the throughput of each algorithm at a given recall value.

% We report the highest throughput value for each algorithm at a recall value of 1.0.
% We report the mean and median speedup factor of CAKES over each algorithm.
% For $k= 10$, we report a mean $7,654 \times$ (median $1,200 \times$) speedup over faiss-ivf, a mean $11,325 \times$ (median $2,821 \times$) speedup over bruteforce-blas, and a mean $485 \times$ (median $417 \times$) speedup over HNSW. 
% For $k=100$, we report a mean $4,479 \times$ (median $3,456 \times$) speedup over faiss-ivf, a mean $16,010 \times$ (median $13,548 \times$) speedup over bruteforce-blas, and a mean $403 \times$ (median $372 \times$) speedup over HNSW.

% For all datasets benchmarked in this manuscript, the auto-tuning method selected repeated $\rho$-nearest neighbor (Section~\ref{subsubsec:methods:knn-search:repeated-rnn}).

% We note that the mean and median speedup factors differ significantly for all methods, which suggests that the speedup factor is heavily dataset-dependent;
% in particular, across all algorithms, and both values of $k$, CAKES exhibits particularly high speedup factors with SIFT and GIST, under Euclidean distance.
% As GIST was designed to be a difficult dataset for classifiers~\cite{Lee2019PracticalLP}, CAKES's strong performance with this dataset is particularly encouraging.
% Further investigation of CAKES's performance on other challenging or adversarial datasets is warranted. 




\begin{table*}[!t]
    % \renewcommand{\arraystretch}{1.15}
    \caption{Runtime performance (queries per second) and recall of CAKES vs. other methods on the fashion-mnist dataset. We use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider only three decimal places.}
    \label{table:results:ann-fashion}
    \vskip 0.15in
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{|l|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|}
                    \hline
                    \textbf{Scale}  & \multicolumn{2}{|c|}{\textbf{hnsw}} & \multicolumn{2}{|c|}{\textbf{annoy}} & \multicolumn{2}{|c|}{\textbf{faiss-flat}} & \multicolumn{2}{|c|}{\textbf{faiss-ivf}}  & \multicolumn{2}{|c|}{\textbf{CAKES}} \\
                    \hline
                    &             QPS & Recall        & QPS & Recall      & QPS & Recall       & QPS & Recall     & QPS & Recall    \\
                    \hline
                    1   & 1.33~$\times10^{4}$ & 0.954 & 2.19~$\times10^{3}$ & 0.950 & 2.13~$\times10^{2}$  & 1.000 & 2.01~$\times10^{3}$ & 1.000* & 2.17~$\times10^{3}$ & 1.000 \\
                    \hline
                    2   & 1.38~$\times10^{4}$ & 0.803 & 2.12~$\times10^{3}$ & 0.927 & 1.06~$\times10^{2}$  & 1.000 & 9.39~$\times10^{2}$ & 1.000* & 1.14~$\times10^{3}$ & 1.000 \\
                    \hline
                    4   & 1.66~$\times10^{4}$ & 0.681 & 2.04~$\times10^{3}$ & 0.898 & 5.31~$\times10^{1}$  & 1.000 & 4.61~$\times10^{2}$ & 0.997  & 9.82~$\times10^{2}$ & 1.000 \\
                    \hline
                    8   & 1.68~$\times10^{4}$ & 0.525 & 1.93~$\times10^{3}$ & 0.857 & 2.66~$\times10^{1}$  & 1.000 & 2.26~$\times10^{2}$ & 0.995  & 1.18~$\times10^{3}$ & 1.000 \\
                    \hline
                    16  & 1.87~$\times10^{4}$ & 0.494 & 1.84~$\times10^{3}$ & 0.862 & 1.33~$\times10^{1}$  & 1.000 & 1.17~$\times10^{2}$ & 0.991  & 1.20~$\times10^{3}$ & 1.000 \\
                    \hline
                    32  & 1.56~$\times10^{4}$ & 0.542 & 1.85~$\times10^{3}$ & 0.775 & 6.65~$\times10^{0}$  & 1.000 & 5.91~$\times10^{1}$ & 0.985  & 1.16~$\times10^{3}$ & 1.000 \\
                    \hline
                    64  & 1.50~$\times10^{4}$ & 0.378 & 1.78~$\times10^{3}$ & 0.677 & 3.32~$\times10^{0}$  & 1.000 & 2.61~$\times10^{1}$ & 0.968  & 1.10~$\times10^{3}$ & 1.000 \\
                    \hline
                    128 & 1.49~$\times10^{4}$ & 0.357 & 1.66~$\times10^{3}$ & 0.538 & 1.66~$\times10^{0}$  & 1.000 & 1.33~$\times10^{1}$ & 0.964  & 1.04~$\times10^{3}$ & 1.000 \\
                    \hline
                    256 & --                  & --    & 1.60~$\times10^{3}$ & 0.592 & 8.31~$\times10^{-1}$ & 1.000 & 6.65~$\times10^{0}$ & 0.962  & 1.06~$\times10^{3}$ & 1.000 \\
                    \hline
                    512 & --                  & --    & 1.83~$\times10^{3}$ & 0.581 & 4.16~$\times10^{-1}$ & 1.000 & 3.56~$\times10^{0}$ & 0.949  & 1.04~$\times10^{3}$ & 1.000 \\
                    \hline
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[!t]
    % \renewcommand{\arraystretch}{1.15}
    \caption{Runtime performance (queries per second) and recall of CAKES vs. other methods on the glove-25 dataset. We use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider only three decimal places.}
    \label{table:results:ann-glove-25}
    \vskip 0.15in
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{|l|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|}
                    \hline
                    \textbf{Scale}  & \multicolumn{2}{|c|}{\textbf{hnsw}} & \multicolumn{2}{|c|}{\textbf{annoy}} & \multicolumn{2}{|c|}{\textbf{faiss-flat}} & \multicolumn{2}{|c|}{\textbf{faiss-ivf}}  & \multicolumn{2}{|c|}{\textbf{CAKES}} \\
                    \hline
                    &             QPS & Recall        & QPS & Recall      & QPS & Recall       & QPS & Recall     & QPS & Recall    \\
                    \hline
                    1   & 2.28~$\times10^{4}$ & 0.801 & 2.83~$\times10^{3}$ & 0.835 & 2.78~$\times10^{2}$ & 1.000 & 2.38~$\times10^{3}$ & 1.000* & 7.22~$\times10^{2}$ & 1.000 \\
                    \hline
                    2   & 2.38~$\times10^{4}$ & 0.607 & 2.70~$\times10^{3}$ & 0.832 & 1.39~$\times10^{2}$ & 1.000 & 1.19~$\times10^{3}$ & 1.000* & 5.75~$\times10^{2}$ & 1.000 \\
                    \hline
                    4   & 2.50~$\times10^{4}$ & 0.443 & 2.61~$\times10^{3}$ & 0.839 & 6.98~$\times10^{1}$ & 1.000 & 6.19~$\times10^{2}$ & 1.000* & 6.25~$\times10^{2}$ & 1.000 \\
                    \hline
                    8   & 2.78~$\times10^{4}$ & 0.294 & 2.51~$\times10^{3}$ & 0.834 & 3.49~$\times10^{1}$ & 1.000 & 3.03~$\times10^{2}$ & 1.000* & 5.93~$\times10^{2}$ & 1.000 \\
                    \hline
                    16  & 3.11~$\times10^{4}$ & 0.213 & 2.23~$\times10^{3}$ & 0.885 & 1.74~$\times10^{1}$ & 1.000 & 1.51~$\times10^{2}$ & 1.000* & 5.49~$\times10^{2}$ & 1.000 \\
                    \hline
                    32  & 3.24~$\times10^{4}$ & 0.178 & 2.01~$\times10^{3}$ & 0.764 & 8.70~$\times10^{0}$ & 1.000 & 7.40~$\times10^{1}$ & 0.999  & 4.75~$\times10^{2}$ & 1.000 \\
                    \hline
                    64  & --                  & --    & 1.99~$\times10^{3}$ & 0.631 & 4.36~$\times10^{0}$ & 1.000 & 3.77~$\times10^{1}$ & 0.997  & 4.61~$\times10^{2}$ & 1.000 \\
                    \hline
                    128 & --                  & --    & --                  & --    & 2.18~$\times10^{0}$ & 1.000 & 1.90~$\times10^{1}$ & 0.998  & 4.41~$\times10^{2}$ & 1.000 \\
                    \hline
                    256 & --                  & --    & --                  & --    & 1.07~$\times10^{0}$ & 1.000 & 9.47~$\times10^{0}$ & 0.998  & 4.23~$\times10^{2}$ & 1.000 \\
                    \hline
                    % 512 & -- & -- & -- & -- & 0.544 & 1.000 & 4.499 & 0.996 & 0.000 & 0.000 \\
                    % \hline
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

\begin{table*}[!t]
    % \renewcommand{\arraystretch}{1.15}
    \caption{Runtime performance (queries per second) and recall of CAKES vs. other methods on the sift dataset. We use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider only three decimal places.}
    \label{table:results:ann-sift}
    \vskip 0.15in
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{|l|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|}
                    \hline
                    \textbf{Scale}  & \multicolumn{2}{|c|}{\textbf{hnsw}}  & \multicolumn{2}{|c|}{\textbf{annoy}} & \multicolumn{2}{|c|}{\textbf{faiss-flat}} & \multicolumn{2}{|c|}{\textbf{faiss-ivf-flat}}  & \multicolumn{2}{|c|}{\textbf{CAKES}} \\
                    \hline
                    &             QPS & Recall        & QPS & Recall      & QPS & Recall       & QPS & Recall    & QPS & Recall    \\
                    \hline
                    1   & 1.93~$\times10^{4}$ & 0.782 & 3.98~$\times10^{3}$ & 0.686 & 7.26~$\times10^{1}$  & 1.000 & 6.98~$\times10^{2}$ & 1.000* & 5.52~$\times10^{2}$ & 1.000 \\
                    \hline
                    2   & 2.03~$\times10^{4}$ & 0.552 & 3.80~$\times10^{3}$ & 0.614 & 3.62~$\times10^{1}$  & 1.000 & 3.30~$\times10^{2}$ & 1.000* & 2.66~$\times10^{2}$ & 1.000 \\
                    \hline
                    4   & 2.18~$\times10^{4}$ & 0.394 & 3.69~$\times10^{3}$ & 0.637 & 1.82~$\times10^{1}$  & 1.000 & 1.65~$\times10^{2}$ & 1.000* & 1.43~$\times10^{2}$ & 1.000 \\
                    \hline
                    8   & 2.48~$\times10^{4}$ & 0.298 & 3.58~$\times10^{3}$ & 0.710 & 9.08~$\times10^{0}$  & 1.000 & 7.72~$\times10^{1}$ & 1.000* & 7.94~$\times10^{1}$ & 1.000 \\
                    \hline
                    16  & 2.68~$\times10^{4}$ & 0.210 & 3.50~$\times10^{3}$ & 0.690 & 4.54~$\times10^{0}$  & 1.000 & 3.98~$\times10^{1}$ & 1.000* & 8.12~$\times10^{1}$ & 1.000 \\
                    \hline
                    32  & 2.75~$\times10^{4}$ & 0.193 & 3.44~$\times10^{3}$ & 0.639 & 2.27~$\times10^{0}$  & 1.000 & 2.09~$\times10^{1}$ & 0.999  & 7.81~$\times10^{1}$ & 1.000 \\
                    \hline
                    64  & --                  & --    & 3.39~$\times10^{3}$ & 0.678 & 1.14~$\times10^{0}$  & 1.000 & 8.87~$\times10^{0}$ & 0.997  & 7.43~$\times10^{1}$ & 1.000 \\
                    \hline
                    128 & --                  & --    & 3.36~$\times10^{3}$ & 0.643 & 5.66~$\times10^{-1}$ & 1.000 & 4.78~$\times10^{0}$ & 0.993  & 6.80~$\times10^{1}$ & 1.000 \\
                    \hline
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[!t]
    % \renewcommand{\arraystretch}{1.15}
    \caption{Runtime performance (queries per second) and recall of CAKES vs. other methods on a random dataset. We use 1.000* to denote that the recall is imperfect, but rounds to 1.000 when we consider only three decimal places.}
    \label{table:results:ann-random}
    \vskip 0.15in
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{|l|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|p{1.2cm}|p{0.8cm}|}
                    \hline
                    \textbf{Scale}  & \multicolumn{2}{|c|}{\textbf{hnsw}}  & \multicolumn{2}{|c|}{\textbf{annoy}} & \multicolumn{2}{|c|}{\textbf{faiss-flat}} & \multicolumn{2}{|c|}{\textbf{faiss-ivf-flat}}  & \multicolumn{2}{|c|}{\textbf{CAKES}} \\
                    \hline
                    &             QPS & Recall        & QPS & Recall      & QPS & Recall       & QPS & Recall    & QPS & Recall    \\
                    \hline
                    1  & 1.17~$\times10^{4}$ & 0.060 & 4.28~$\times10^{3}$ & 0.028 & 7.16~$\times10^{1}$ & 1.000 & 7.34~$\times10^{2}$ & 1.000* & 5.54~$\times10^{2}$ & 1.000 \\
                    \hline
                    2  & 1.01~$\times10^{4}$ & 0.048 & 4.04~$\times10^{3}$ & 0.021 & 3.62~$\times10^{1}$ & 1.000 & 3.58~$\times10^{2}$ & 1.000* & 2.69~$\times10^{2}$ & 1.000 \\
                    \hline
                    4  & 9.12~$\times10^{3}$ & 0.031 & 3.64~$\times10^{3}$ & 0.014 & 1.81~$\times10^{1}$ & 1.000 & 1.90~$\times10^{2}$ & 1.000* & 1.37~$\times10^{2}$ & 1.000 \\
                    \hline
                    8  & 8.35~$\times10^{3}$ & 0.022 & 3.37~$\times10^{3}$ & 0.013 & 9.04~$\times10^{0}$ & 1.000 & 8.84~$\times10^{1}$ & 1.000* & 5.69~$\times10^{1}$ & 1.000 \\
                    \hline
                    16 & 8.25~$\times10^{3}$ & 0.008 & 3.17~$\times10^{3}$ & 0.006 & 4.53~$\times10^{0}$ & 1.000 & 4.36~$\times10^{1}$ & 1.000* & 2.61~$\times10^{1}$ & 1.000 \\
                    \hline
                    32 & --                  & --    & 3.01~$\times10^{3}$ & 0.007 & 2.27~$\times10^{0}$ & 1.000 & 1.72~$\times10^{1}$ & 1.000* & 1.35~$\times10^{1}$ & 1.000 \\
                    % \hline
                    % 64 & -- & -- & -- & -- & 1.135 & 1.000 & 9.939 & 1.000 & 0.000 & 0.000 \\
                    % \hline
                    % 128 & --& -- & -- & -- & 0.567 & 1.000 & 5.106 & 1.000 & 0.000 & 0.000 \\
                    \hline
                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

% \begin{table*}[!t]
%     % \renewcommand{\arraystretch}{1.15}
%     \caption{Runtime performance (queries per second) of CAKES and Speedup Factor over Naive Linear Search, $k=10$}
%     \label{table:results:ann-alt-10}
%     \vskip 0.15in
%     \begin{center}
%         \begin{small}
%             \begin{sc}
%                 \begin{tabular}{|l|l|l|l|l|}
%                     \hline
%                     \textbf{Dataset} & \textbf{CAKES Throughput} & \textbf{Speedup Factor over Linear} \\
%                     \hline
%                     deep-image       & 232.1                     & 37.2      \\
%                     \hline
%                     mnist            & 129,300                   & 101.9      \\
%                     \hline
%                     glove-50         & 10,950                    & 12.28      \\
%                     \hline 
%                     glove-200        & 2,290                     & 8.55     \\
%                     \hline
%                     lastfm           & 13,950                    & 4.55           \\
%                     \hline
%                 \end{tabular}
%             \end{sc}
%         \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table*}


% \begin{table*}[!t]
%     % \renewcommand{\arraystretch}{1.15}
%     \caption{Runtime performance (queries per second) of CAKES and Speedup Factor over Naive Linear Search, $k=100$}
%     \label{table:results:ann-alt-100}
%     \vskip 0.15in
%     \begin{center}
%         \begin{small}
%             \begin{sc}
%                 \begin{tabular}{|l|l|l|l|l|l|}
%                     \hline
%                     \textbf{Dataset} & \textbf{CAKES Throughput} & \textbf{Speedup Factor over Linear} \\
%                     \hline
%                     deep-image        & 157.3                    & 25.74                               \\                            
%                     \hline
%                     mnist             & 71,090                   & 56.63                               \\
%                     \hline
%                     glove-50          & 8,867                    & 9.84                                \\
%                     \hline 
%                     glove-200         & 2,264                    & 8.49                                \\
%                     \hline
%                     lastfm            & 13,950                   & 4.55                                \\
%                     \hline
%                 \end{tabular}
%             \end{sc}
%         \end{small}
%     \end{center}
%     \vskip -0.1in
% \end{table*}
