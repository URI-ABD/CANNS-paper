\section{Discussion}
\label{sec:discussion}

Discuss results and future work \dots

CLAM-CAKES.
This is most effective when datasets exhibit low metric entropy and low local fractal dimension.

Unlike LSH, this is extensible to any user provided distance function.

Exactness of search and metric vs non-metric.

Is it the case that the optimal clusters for compression are also the optimal clusters for accelerated search?

\subsection{Future Works}
\label{subsec:results:future-works}

GPU-acceleration for the more computationally intensive distance functions like wasserstein.

Live-updates as new points are added to the dataset.
For example, build tree with a subsample of the full data and then add the rest of the instances to simulate live-updates.

More distance functions. For example, maximal-common-subgraph for molecular structures.

Better methods for constructing minimal encodings, using domain expertise.
