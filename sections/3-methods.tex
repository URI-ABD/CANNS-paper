\section{Methods}
\label{sec:methods}

Write about methods and provide asymptotic complexity analysis of each method.

\subsection{Partition}
\label{subsec:methods:partition}

Partition algorithm for building the tree.
Lift this from the CHAODA paper.

Exact partition with $\mathcal{O}(n^2)$ cost vs approximate partition using $\sqrt{n}$ seeds to achieve $\mathcal{O}(n)$ cost.
Building the exact tree costs $\mathcal{O}(n^2 \log n)$ vs approximate tree for $\mathcal{O}(n \log n)$.

\subsection{\texorpdfstring{$\rho$}{p}-Nearest Neighbors Search}
\label{subsec:methods:rnn-search}

Given a query $q$ and a search radius $\rho$, find all $x \in X$ s.t. $f(q, x) \leq \rho$.

Modified binary search to identify candidate leaf clusters.
Exhaustive leaf search over those leaves.

Lift this from CHESS paper. Be sure to cite CHESS here.

Complexity is the same as in CHESS.

\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{subsec:methods:knn-search}

Given a query $q$ and a positive integer $k$, find the $k$ closest instances in $X$.

Given a Cluster $C$, let $c$ be its center and $r$ be its radius.
Let $\delta = f(q, c)$ be the distance from the query to the cluster center.
Let $\delta_{max} = \delta + r$ be the distance from the query to the potentially farthest instance in the Cluster.
Let $\delta_{min} = max(0, \delta - r)$ be the distance from the query to the potentially closest instance in the Cluster.

\subsubsection{Expanding Threshold}
\label{subsubsec:methods:knn-search:expanding-threshold}

Create a priority queue, $candidates$, wherein the top priority candidate is the one with the smallest $\delta_{min}$.
Initialize $candidates$ with the root cluster.
Create a second priority queue, $hits$, wherein the top priority hit is the 
one with the largest $\delta$. This queue starts empty. 

While $hits$ has fewer than $k$ clusters or the $\delta$ of the top priority hit is greater 
than the $\delta_{min}$ of the top priority candidate, do the following:
\begin{enumerate}
\item While the top priority candidate isn't a leaf, replace the top priority 
candidate with its children.
\item Remove the top priority candidate from $candidates$ and add all of its 
points to $hits$. 
\item Remove points from $hits$ until $hits$ has $k$ points. 
\end{enumerate}

Extract hits. 


\begin{algorithm} % enter the algorithm environment
\caption{Expanding Threshold} % give the algorithm a caption
\label{alg:expanding_threshold} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $tree$, $query$, $k$
    \STATE $candidates \leftarrow$ priority queue
    \STATE $hits \leftarrow$ priority queue
    \WHILE{$|hits| < k \lor hits.peek().\delta > candidates.peek().\delta_{min}$}
        \WHILE{$!candidates.peek().isLeaf$}
            \STATE $[l, r] \leftarrow candidates.pop().children()$
            \STATE $candidates.push([l, r])$
        \ENDWHILE
        \STATE $leaf \leftarrow candidates.pop()$
        \STATE $hits.push(leaf)$
        \WHILE{$|hits| > k$}
            \STATE $hits.pop()$
        \ENDWHILE
    \ENDWHILE
    \STATE Return $hits$
\end{algorithmic}
\end{algorithm}


\subsubsection{Sieve V1}
\label{subsubsec:methods:knn-search:sieve-v1}
Add explanation of the sieve v1 algorithm.

\subsubsection{Sieve V2}
\label{subsubsec:methods:knn-search:sieve-v2}
Add explanation of the sieve v2 algorithm.

\subsubsection{Repeated Rnn}
\label{subsubsec:methods:knn-search:repeated-rnn}
Add explanation of the repeated rnn algorithm.

\subsubsection{Leaf Search}
\label{subsubsec:methods:knn-search:leaf-search}

Let $hits$ be a a fixed size priority queue with $k$ isntances sorted by non-decreasing $f(q, x)$, i.e. distance to the query.
Initialize $hits$ with the centers of $candidates$.

Let $f_k$ be the distance to the $k^{th}$ farthest instance so far.
Exclude all $candidates$ for which $\delta^2 > f_k$, i.e. their closest possible instance is farthest that the $k^{th}$ farthest instance found so far.
Remove the closest cluster from $candidates$ and add all its contained instances to $hits$.
Repeat until $candidates$ is empty.

\subsubsection{Complexity}
\label{subsubsec:methods:knn-search:complexity}

This is at-least on-par with the complexity for $\rho$-nearest neighbors search from CHESS.
It is potentially better because we effectively shrink the search radius to adapt to the $k$ closest instances found during the algorithm.

\subsection{Compression}
\label{subsec:methods:compression}

Encode and Decode algorithms.
These could be, and probably will be, different for each $(X, f)$ pairing.

Asymptotic complexity analysis for:
\begin{enumerate}[i.]
    \item encode (novel contribution)
    \item serialize
    \item compress
    \item decompress
    \item deserialize
    \item decode (novel contribution)
\end{enumerate}

% discrete vs continuous, and small alphabet vs large alphabet in discrete
