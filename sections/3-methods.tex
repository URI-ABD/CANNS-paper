\section{Methods}
\label{sec:methods}

Write about methods and provide asymptotic complexity analysis of each method.

\subsection{Partition}
\label{subsec:methods:partition}

Partition algorithm for building the tree.
Lift this from the CHAODA paper.

Exact partition with $\mathcal{O}(n^2)$ cost vs approximate partition using $\sqrt{n}$ seeds to achieve $\mathcal{O}(n)$ cost.
Building the exact tree costs $\mathcal{O}(n^2 \log n)$ vs approximate tree for $\mathcal{O}(n \log n)$.

\subsection{\texorpdfstring{$\rho$}{p}-Nearest Neighbors Search}
\label{subsec:methods:rnn-search}

Given a query $q$ and a search radius $\rho$, find all $x \in X$ s.t. $f(q, x) \leq \rho$.

Modified binary search to identify candidate leaf clusters.
Exhaustive leaf search over those leaves.

Lift this from CHESS paper. Be sure to cite CHESS here.

Complexity is the same as in CHESS.

\subsection{\texorpdfstring{$k$}{k}-Nearest Neighbors Search}
\label{subsec:methods:knn-search}

Given a query $q$ and a positive integer $k$, find the $k$ closest instances in $X$.

Given a Cluster $C$, let $c$ be its center and $r$ be its radius.
Let $\delta^0 = f(q, c)$ be the distance from the query to the cluster center.
Let $\delta^1 = \delta^0 + r$ be the distance from the query to the potentially farthest instance in the Cluster.
Let $\delta^2 = max(0, \delta^0 - r)$ be the distance from the query to the potentially closest instance in the Cluster.

\subsubsection{Tree Search}
\label{subsubsec:methods:knn-search:tree-search}

Add each cluster to a priority queue $m$ times, where $m$ is the cardinality of the cluster, and the priority is the $\delta^1$ distance of that cluster. 
Then, any cluster whose $\delta^2$ is greater than the $k^{th}$ highest $\delta^1$.

Let $candidates$ be a priority queue sorted by non-decreasing $\delta^1$.
Initialize $candidates$ with the root cluster.
Replace clusters in $candidates$ by their children until $candidates$ has at least $k$ clusters.

Take the $k^{th}$ candidate.
Any cluster with $\delta^2 \leq \delta^1_k$ could contain one of the $k$-nearest neighbors of $q$.
Keep all such clusters in $candidates$ and exclude all other clusters.
Expand search to the children of all $candidates$ so far and repeat until all $candidates$ are leaf clusters.

Sort, as a priority queue, $candidates$ by $\delta^2$ and break ties by $\delta^0$.
Proceed to Leaf Search with these candidates.

\subsubsection{Leaf Search}
\label{subsubsec:methods:knn-search:leaf-search}

Let $hits$ be a a fixed size priority queue with $k$ isntances sorted by non-decreasing $f(q, x)$, i.e. distance to the query.
Initialize $hits$ with the centers of $candidates$.

Let $f_k$ be the distance to the $k^{th}$ farthest instance so far.
Exclude all $candidates$ for which $\delta^2 > f_k$, i.e. their closest possible instance is farthest that the $k^{th}$ farthest instance found so far.
Remove the closest cluster from $candidates$ and add all its contained instances to $hits$.
Repeat until $candidates$ is empty.

\subsubsection{Complexity}
\label{subsubsec:methods:knn-search:complexity}

This is at-least on-par with the complexity for $\rho$-nearest neighbors search from CHESS.
It is potentially better because we effectively shrink the search radius to adapt to the $k$ closest instances found during the algorithm.

\subsection{Compression}
\label{subsec:methods:compression}

Encode and Decode algorithms.
These could be, and probably will be, different for each $(X, f)$ pairing.

Asymptotic complexity analysis for:
\begin{enumerate}[i.]
    \item encode (novel contribution)
    \item serialize
    \item compress
    \item decompress
    \item deserialize
    \item decode (novel contribution)
\end{enumerate}

% discrete vs continuous, and small alphabet vs large alphabet in discrete
